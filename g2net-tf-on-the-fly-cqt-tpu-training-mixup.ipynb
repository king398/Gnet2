{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## About this notebook\n",
    "\n",
    "This notebook is based on [CQT G2Net EfficientNetB1[TPU Training]](https://www.kaggle.com/miklgr500/cqt-g2net-efficientnetb7-tpu-training-w-b) by [Welf Crozzo](https://www.kaggle.com/miklgr500) and [nnAudio Constant Q-transform Demonstration](https://www.kaggle.com/atamazian/nnaudio-constant-q-transform-demonstration) by [Araik Tamazian](https://www.kaggle.com/atamazian).\n",
    "\n",
    "This notebook use Constant Q-Transform for feature extraction and EfficientNetB0 for classification. The whole pipeline is implemented with Tensorflow, and the training process runs on TPU.\n",
    "\n",
    "The main difference between this notebook and Welf's notebook is the use of on-the-fly CQT computation implemented with Tensorflow, which is similar to the idea of [nnAudio](https://github.com/KinWaiCheuk/nnAudio)'s [CQT1992v2](https://kinwaicheuk.github.io/nnAudio/_autosummary/nnAudio.Spectrogram.CQT1992v2.html?highlight=cqt1992v2#nnAudio.Spectrogram.CQT1992v2) layer.\n",
    "\n",
    "* [Inference Notebook](https://www.kaggle.com/hidehisaarai1213/g2net-tf-on-the-fly-cqt-tpu-inference)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install Dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install efficientnet tensorflow_addons > /dev/null"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:17.353826Z",
     "iopub.execute_input": "2021-09-02T17:04:17.354176Z",
     "iopub.status.idle": "2021-09-02T17:04:26.772116Z",
     "shell.execute_reply.started": "2021-09-02T17:04:17.354143Z",
     "shell.execute_reply": "2021-09-02T17:04:26.771183Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import efficientnet.tfkeras as efn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from scipy.signal import get_window\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:26.77386Z",
     "iopub.execute_input": "2021-09-02T17:04:26.774194Z",
     "iopub.status.idle": "2021-09-02T17:04:34.504938Z",
     "shell.execute_reply.started": "2021-09-02T17:04:26.774154Z",
     "shell.execute_reply": "2021-09-02T17:04:34.503987Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tf.__version__"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:34.506555Z",
     "iopub.execute_input": "2021-09-02T17:04:34.506843Z",
     "iopub.status.idle": "2021-09-02T17:04:34.515024Z",
     "shell.execute_reply.started": "2021-09-02T17:04:34.506816Z",
     "shell.execute_reply": "2021-09-02T17:04:34.513987Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Config"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "NUM_FOLDS = 4\n",
    "#IMAGE_SIZE = 456#change\n",
    "IMAGE_SIZE = 512#change\n",
    "BATCH_SIZE = 32\n",
    "EFFICIENTNET_SIZE =7\n",
    "WEIGHTS = \"imagenet\"\n",
    "\n",
    "MIXUP_PROB = 0.0#change\n",
    "EPOCHS = 20\n",
    "R_ANGLE = 0 / 180 * np.pi\n",
    "S_SHIFT = 0.0\n",
    "T_SHIFT = 0.0\n",
    "LABEL_POSITIVE_SHIFT = 0.95"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:34.517049Z",
     "iopub.execute_input": "2021-09-02T17:04:34.517679Z",
     "iopub.status.idle": "2021-09-02T17:04:34.527493Z",
     "shell.execute_reply.started": "2021-09-02T17:04:34.517632Z",
     "shell.execute_reply": "2021-09-02T17:04:34.526549Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "SAVEDIR = Path(\"models\")\n",
    "SAVEDIR.mkdir(exist_ok=True)\n",
    "\n",
    "OOFDIR = Path(\"oof\")\n",
    "OOFDIR.mkdir(exist_ok=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:34.529209Z",
     "iopub.execute_input": "2021-09-02T17:04:34.529709Z",
     "iopub.status.idle": "2021-09-02T17:04:34.540339Z",
     "shell.execute_reply.started": "2021-09-02T17:04:34.529627Z",
     "shell.execute_reply": "2021-09-02T17:04:34.539264Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utilities"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "set_seed(1213)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:34.542332Z",
     "iopub.execute_input": "2021-09-02T17:04:34.542821Z",
     "iopub.status.idle": "2021-09-02T17:04:34.551806Z",
     "shell.execute_reply.started": "2021-09-02T17:04:34.542724Z",
     "shell.execute_reply": "2021-09-02T17:04:34.550739Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def auto_select_accelerator():\n",
    "    TPU_DETECTED = False\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "        print(\"Running on TPU:\", tpu.master())\n",
    "        TPU_DETECTED = True\n",
    "    except ValueError:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
    "\n",
    "    return strategy, TPU_DETECTED"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:34.553537Z",
     "iopub.execute_input": "2021-09-02T17:04:34.554131Z",
     "iopub.status.idle": "2021-09-02T17:04:34.56375Z",
     "shell.execute_reply.started": "2021-09-02T17:04:34.554082Z",
     "shell.execute_reply": "2021-09-02T17:04:34.56284Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "strategy, tpu_detected = auto_select_accelerator()\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:34.567399Z",
     "iopub.execute_input": "2021-09-02T17:04:34.567781Z",
     "iopub.status.idle": "2021-09-02T17:04:40.546062Z",
     "shell.execute_reply.started": "2021-09-02T17:04:34.567735Z",
     "shell.execute_reply": "2021-09-02T17:04:40.544559Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:40.547657Z",
     "iopub.execute_input": "2021-09-02T17:04:40.54793Z",
     "iopub.status.idle": "2021-09-02T17:04:41.998291Z",
     "shell.execute_reply.started": "2021-09-02T17:04:40.547903Z",
     "shell.execute_reply": "2021-09-02T17:04:41.997541Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "GCS_PATH1 = \"gs://kds-0ce57d316a43123380114b5d7ad7220bd2d98bc2d2ca3c6b12066dcb\"\n",
    "GCS_PATH2 = \"gs://kds-00963e1b0402e5e11824a284b3c7b57cc1aa90a7f848ca0f70ea02c1\"\n",
    "GCS_PATH3 = \"gs://kds-0a1b764bbf29152f53ab0a3271970056596514c6dc3c05f74e5bc3ad\"\n",
    "# Data access (Test tf records)\n",
    "GCS_PATH4 = \"gs://kds-0f37e33fcfe9fece00f03f55c79d28611c576e844d1293ba786136ea\"\n",
    "GCS_PATH5 = \"gs://kds-cc48964f90b8da9c9d79a6c8c500e7aae7fd9d6dc69f243deb29b5c9\"\n",
    "print(GCS_PATH1, GCS_PATH2, GCS_PATH3, GCS_PATH4, GCS_PATH5)\n",
    "# Configuration\n",
    "\n",
    "# Training filenames directory\n",
    "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH1 + '/train*.tfrec') + tf.io.gfile.glob(\n",
    "\tGCS_PATH2 + '/train*.tfrec') + tf.io.gfile.glob(GCS_PATH3 + '/train*.tfrec')\n",
    "# Testing filenames directory\n",
    "\n",
    "all_files = TRAINING_FILENAMES\n",
    "\n",
    "\n",
    "print(\"train_files: \", len(all_files))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:41.999292Z",
     "iopub.execute_input": "2021-09-02T17:04:41.999707Z",
     "iopub.status.idle": "2021-09-02T17:04:42.295692Z",
     "shell.execute_reply.started": "2021-09-02T17:04:41.999677Z",
     "shell.execute_reply": "2021-09-02T17:04:42.294652Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Dataset Preparation\n\nHere's the main contribution of this notebook - Tensorflow version of on-the-fly CQT computation. Note that some of the operations used in CQT computation are not supported by TPU, therefore the implementation is not a TF layer but a function that runs on CPU.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def create_cqt_kernels(\n    q: float,\n    fs: float,\n    fmin: float,\n    n_bins: int = 84,\n    bins_per_octave: int = 12,\n    norm: float = 1,\n    window: str = \"hann\",\n    fmax: Optional[float] = None,\n    topbin_check: bool = True\n) -> Tuple[np.ndarray, int, np.ndarray, float]:\n    fft_len = 2 ** _nextpow2(np.ceil(q * fs / fmin))\n    \n    if (fmax is not None) and (n_bins is None):\n        n_bins = np.ceil(bins_per_octave * np.log2(fmax / fmin))\n        freqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n    elif (fmax is None) and (n_bins is not None):\n        freqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n    else:\n        warnings.warn(\"If nmax is given, n_bins will be ignored\", SyntaxWarning)\n        n_bins = np.ceil(bins_per_octave * np.log2(fmax / fmin))\n        freqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n        \n    if np.max(freqs) > fs / 2 and topbin_check:\n        raise ValueError(f\"The top bin {np.max(freqs)} Hz has exceeded the Nyquist frequency, \\\n                           please reduce the `n_bins`\")\n    \n    kernel = np.zeros((int(n_bins), int(fft_len)), dtype=np.complex64)\n    \n    length = np.ceil(q * fs / freqs)\n    for k in range(0, int(n_bins)):\n        freq = freqs[k]\n        l = np.ceil(q * fs / freq)\n        \n        if l % 2 == 1:\n            start = int(np.ceil(fft_len / 2.0 - l / 2.0)) - 1\n        else:\n            start = int(np.ceil(fft_len / 2.0 - l / 2.0))\n\n        sig = get_window(window, int(l), fftbins=True) * np.exp(\n            np.r_[-l // 2:l // 2] * 1j * 2 * np.pi * freq / fs) / l\n        \n        if norm:\n            kernel[k, start:start + int(l)] = sig / np.linalg.norm(sig, norm)\n        else:\n            kernel[k, start:start + int(l)] = sig\n    return kernel, fft_len, length, freqs\n\n\ndef _nextpow2(a: float) -> int:\n    return int(np.ceil(np.log2(a)))\n\n\ndef prepare_cqt_kernel(\n    sr=22050,\n    hop_length=512,\n    fmin=32.70,\n    fmax=None,\n    n_bins=84,\n    bins_per_octave=12,\n    norm=1,\n    filter_scale=1,\n    window=\"hann\"\n):\n    q = float(filter_scale) / (2 ** (1 / bins_per_octave) - 1)\n    print(q)\n    return create_cqt_kernels(q, sr, fmin, n_bins, bins_per_octave, norm, window, fmax)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:42.299041Z",
     "iopub.execute_input": "2021-09-02T17:04:42.299421Z",
     "iopub.status.idle": "2021-09-02T17:04:42.318474Z",
     "shell.execute_reply.started": "2021-09-02T17:04:42.299386Z",
     "shell.execute_reply": "2021-09-02T17:04:42.316998Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "cqt_kernels, KERNEL_WIDTH, lengths, _ = prepare_cqt_kernel(\n    sr=2048,\n    hop_length=HOP_LENGTH,\n    fmin=20,\n    #fmin=24, #change\n    #fmax=1024,\n    fmax=768, #change\n    bins_per_octave=24)#img size 324 dint yield better results 8660",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#HOP_LENGTH = 16\nHOP_LENGTH = 12 #change\ncqt_kernels, KERNEL_WIDTH, lengths, _ = prepare_cqt_kernel(\n    sr=2048,\n    hop_length=HOP_LENGTH,\n    fmin=20,#change\n    #fmin=24, #change\n    fmax=1024,\n    \n    bins_per_octave=12) #change\nprint(KERNEL_WIDTH, len(lengths))\nLENGTHS = tf.constant(lengths, dtype=tf.float32)\nCQT_KERNELS_REAL = tf.constant(np.swapaxes(cqt_kernels.real[:, np.newaxis, :], 0, 2))\nCQT_KERNELS_IMAG = tf.constant(np.swapaxes(cqt_kernels.imag[:, np.newaxis, :], 0, 2))\nprint(CQT_KERNELS_REAL.shape )\nPADDING = tf.constant([[0, 0],\n                        [KERNEL_WIDTH // 2, KERNEL_WIDTH // 2],\n                        [0, 0]])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:42.320014Z",
     "iopub.execute_input": "2021-09-02T17:04:42.32053Z",
     "iopub.status.idle": "2021-09-02T17:04:42.360407Z",
     "shell.execute_reply.started": "2021-09-02T17:04:42.320481Z",
     "shell.execute_reply": "2021-09-02T17:04:42.359437Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def create_cqt_image(wave, hop_length=16):\n    CQTs = []\n    for i in range(3):\n        x = wave[i]\n        #print(x.shape)\n        x = tf.expand_dims(tf.expand_dims(x, 0), 2)\n        #print(x.shape)\n        x = tf.pad(x, PADDING, \"REFLECT\")\n        #print('pad',x.shape)\n        CQT_real = tf.nn.conv1d(x, CQT_KERNELS_REAL, stride=hop_length, padding=\"VALID\")\n        \n        CQT_imag = -tf.nn.conv1d(x, CQT_KERNELS_IMAG, stride=hop_length, padding=\"VALID\")\n        #print('cqt',x.shape,CQT_real.shape)\n        #print(LENGTHS.shape)\n        #print(u)\n        CQT_real *= tf.math.sqrt(LENGTHS)\n        CQT_imag *= tf.math.sqrt(LENGTHS)\n\n        CQT = tf.math.sqrt(tf.pow(CQT_real, 2) + tf.pow(CQT_imag, 2))\n        CQTs.append(CQT[0])\n    return tf.stack(CQTs, axis=2)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:42.361581Z",
     "iopub.execute_input": "2021-09-02T17:04:42.3619Z",
     "iopub.status.idle": "2021-09-02T17:04:42.370262Z",
     "shell.execute_reply.started": "2021-09-02T17:04:42.361868Z",
     "shell.execute_reply": "2021-09-02T17:04:42.369191Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "'''\nWINDOW = tf.cast(tf.constant(get_window(('tukey', 0.1), 4096)), tf.float32)\n\ndef create_cqt_image(wave, hop_length=HOP_LENGTH, window=WINDOW):\n    x = wave * window\n    x = tf.expand_dims(x, 2)\n    x = tf.pad(x, PADDING, \"REFLECT\")\n    cqt_real = tf.nn.conv1d(x, CQT_KERNELS_REAL, stride=hop_length, padding=\"VALID\")\n    cqt_imag = -tf.nn.conv1d(x, CQT_KERNELS_IMAG, stride=hop_length, padding=\"VALID\")\n    cqt_real *= tf.math.sqrt(LENGTHS)\n    cqt_imag *= tf.math.sqrt(LENGTHS)\n    cqt_amlp = tf.math.sqrt(tf.pow(cqt_real, 2) + tf.pow(cqt_imag, 2))\n    #cqt_amlp = cqt_amlp / tf.expand_dims(tf.reduce_mean(cqt_amlp, axis=1), 1)\n    return tf.transpose(cqt_amlp, perm=[2, 1, 0])\n'''",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:42.371447Z",
     "iopub.execute_input": "2021-09-02T17:04:42.371741Z",
     "iopub.status.idle": "2021-09-02T17:04:42.390743Z",
     "shell.execute_reply.started": "2021-09-02T17:04:42.371713Z",
     "shell.execute_reply": "2021-09-02T17:04:42.38943Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def read_labeled_tfrecord(example):\n    tfrec_format = {\n        \"wave\": tf.io.FixedLenFeature([], tf.string),\n        \"wave_id\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    }\n    example = tf.io.parse_single_example(example, tfrec_format)\n    '''\n    if valid:\n        \n        return prepare_image(example[\"wave\"], IMAGE_SIZE), tf.reshape(tf.cast(example[\"target\"], tf.float32), [1]), example[\"wave_id\"]\n    else:\n        return prepare_image(example[\"wave\"], IMAGE_SIZE), tf.reshape(tf.cast(example[\"target\"], tf.float32), [1])\n    '''\n    return prepare_image(example[\"wave\"], IMAGE_SIZE), tf.reshape(tf.cast(example[\"target\"], tf.float32), [1]) \n \n\ndef read_unlabeled_tfrecord(example, return_image_id):\n    tfrec_format = {\n        \"wave\": tf.io.FixedLenFeature([], tf.string),\n        \"wave_id\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrec_format)\n    return prepare_image(example[\"wave\"], IMAGE_SIZE), example[\"wave_id\"] if return_image_id else 0\n\n\ndef count_data_items(fileids):\n    return len(fileids) * 28000\n\n\ndef count_data_items_test(fileids):\n    return len(fileids) * 22600\n\n\ndef mixup(image, label, probability=0.5, aug_batch=64 * 8):\n    imgs = []\n    labs = []\n    for j in range(aug_batch):\n        p = tf.cast(tf.random.uniform([], 0, 1) <= probability, tf.float32)\n        k = tf.cast(tf.random.uniform([], 0, aug_batch), tf.int32)\n        a = tf.random.uniform([], 0, 1) * p\n\n        img1 = image[j]\n        img2 = image[k]\n        imgs.append((1 - a) * img1 + a * img2)\n        #lab1 = label[j]\n        #lab2 = label[k]\n        lab1 = label[j]*LABEL_POSITIVE_SHIFT if label[j]!=0 else  label[j]*(1-LABEL_POSITIVE_SHIFT)\n        lab2 = label[k]*LABEL_POSITIVE_SHIFT if label[k]!=0 else  label[k]*(1-LABEL_POSITIVE_SHIFT)\n        labs.append((1 - a) * lab1 + a * lab2)\n    image2 = tf.reshape(tf.stack(imgs), (aug_batch, IMAGE_SIZE, IMAGE_SIZE, 3))\n    label2 = tf.reshape(tf.stack(labs), (aug_batch,))\n    return image2, label2\n\n\ndef time_shift(img, shift=T_SHIFT):\n    if shift > 0:\n        T = IMAGE_SIZE\n        P = tf.random.uniform([],0,1)\n        SHIFT = tf.cast(T * P, tf.int32)\n        return tf.concat([img[-SHIFT:], img[:-SHIFT]], axis=0)\n    return img\n\n\ndef rotate(img, angle=R_ANGLE):\n    if angle > 0:\n        P = tf.random.uniform([],0,1)\n        A = tf.cast(angle * P, tf.float32)\n        return tfa.image.rotate(img, A)\n    return img\n\n\ndef spector_shift(img, shift=S_SHIFT):\n    if shift > 0:\n        T = IMAGE_SIZE\n        P = tf.random.uniform([],0,1)\n        SHIFT = tf.cast(T * P, tf.int32)\n        return tf.concat([img[:, -SHIFT:], img[:, :-SHIFT]], axis=1)\n    return img\n\ndef img_aug_f(img):\n    img = time_shift(img)\n    img = spector_shift(img)\n    # img = rotate(img)\n    return img\n\n\ndef imgs_aug_f(imgs, batch_size):\n    _imgs = []\n    DIM = IMAGE_SIZE\n    for j in range(batch_size):\n        _imgs.append(img_aug_f(imgs[j]))\n    return tf.reshape(tf.stack(_imgs),(batch_size,DIM,DIM,3))\n\n\ndef label_positive_shift(labels):\n    return labels * LABEL_POSITIVE_SHIFT\n\n\ndef aug_f(imgs, labels, batch_size):\n    imgs, label = mixup(imgs, labels, MIXUP_PROB, batch_size)\n    imgs = imgs_aug_f(imgs, batch_size)\n    #return imgs, label_positive_shift(label)\n    return imgs, label\n\n\ndef prepare_image(wave, dim=512):\n\t# Decode raw\n\twave = tf.reshape(tf.io.decode_raw(wave, tf.float64), (3, 4096))\n\tscaling = tf.constant([1.5e-20, 1.5e-20, 0.5e-20], dtype=tf.float64)\n\n\tnormalized_waves = []\n\t# Normalize\n\tfor i in range(3):\n\t\tnormalized_wave = wave[i] / scaling[i]\n\t\tnormalized_waves.append(normalized_wave)\n\n\twave = tf.stack(normalized_waves)\n\twave = tf.cast(wave, tf.float32)\n\timage = create_cqt_image(wave, HOP_LENGTH)\n\timage = tf.image.resize(image, size=(dim, dim))\n\treturn tf.reshape(image, (dim, dim, 3))\n\n\ndef get_dataset(files, batch_size=16, repeat=False, shuffle=False, aug=True, \n                labeled=True, return_image_ids=True,valid=False):\n    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO, compression_type=\"GZIP\")\n    ds = ds.cache()\n\n    if repeat:\n        ds = ds.repeat()\n\n    if shuffle:\n        ds = ds.shuffle(1024 * 2)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n\n    if labeled:\n        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n        #ds = ds.map(lambda example:read_labeled_tfrecord(example), num_parallel_calls=AUTO)\n    else:\n        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_ids), num_parallel_calls=AUTO)\n\n    ds = ds.batch(batch_size * REPLICAS)\n    if aug:\n        ds = ds.map(lambda x, y: aug_f(x, y, batch_size * REPLICAS), \n                    num_parallel_calls=AUTO)\n    ds = ds.prefetch(AUTO)\n    return ds",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:42.392073Z",
     "iopub.execute_input": "2021-09-02T17:04:42.392416Z",
     "iopub.status.idle": "2021-09-02T17:04:42.426572Z",
     "shell.execute_reply.started": "2021-09-02T17:04:42.392353Z",
     "shell.execute_reply": "2021-09-02T17:04:42.42547Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def build_model(size=256, efficientnet_size=0, weights=\"imagenet\", count=0):\n    inputs = tf.keras.layers.Input(shape=(size, size, 3))\n    \n    efn_string= f\"EfficientNetB{efficientnet_size}\"\n    efn_layer = getattr(efn, efn_string)(input_shape=(size, size, 3), weights=weights, include_top=False)\n\n    x = efn_layer(inputs)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n\n    x = tf.keras.layers.Dropout(0.3)(x)#change\n    x = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n    model = tf.keras.Model(inputs=inputs, outputs=x)\n\n    lr_decayed_fn = tf.keras.experimental.CosineDecay(1e-3, count)\n    opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate=1e-4)\n    loss = tf.keras.losses.BinaryCrossentropy()\n    model.compile(optimizer=opt, loss=loss, metrics=[\"AUC\"])\n    return model",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:42.427888Z",
     "iopub.execute_input": "2021-09-02T17:04:42.428188Z",
     "iopub.status.idle": "2021-09-02T17:04:42.44478Z",
     "shell.execute_reply.started": "2021-09-02T17:04:42.428157Z",
     "shell.execute_reply": "2021-09-02T17:04:42.443622Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "0.000015 * 8 * 32",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:42.446064Z",
     "iopub.execute_input": "2021-09-02T17:04:42.446395Z",
     "iopub.status.idle": "2021-09-02T17:04:42.461304Z",
     "shell.execute_reply.started": "2021-09-02T17:04:42.446343Z",
     "shell.execute_reply": "2021-09-02T17:04:42.460262Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def get_lr_callback(batch_size=8, replicas=8,resize=False):\n    if  resize:\n        lr_start=5e-5\n        lr_max=3e-3\n        lr_min=3e-5\n        lr_ramp_ep = 8\n    else:\n        \n        lr_start   = 4e-5\n        #lr_max     = 0.000015 * replicas * batch_size\n        lr_max     = 4e-3\n        lr_min     = 2e-5#change\n        lr_ramp_ep = 6 #change\n    lr_sus_ep  = 0\n    lr_decay   = 0.7\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n    return lr_callback",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:42.462626Z",
     "iopub.execute_input": "2021-09-02T17:04:42.462942Z",
     "iopub.status.idle": "2021-09-02T17:04:42.474204Z",
     "shell.execute_reply.started": "2021-09-02T17:04:42.462911Z",
     "shell.execute_reply": "2021-09-02T17:04:42.473081Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Training",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=1213)\noof_pred = []\noof_target = []\noof_img_ids=[]\n\nfiles_train_all = np.array(all_files)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:42.475317Z",
     "iopub.execute_input": "2021-09-02T17:04:42.475705Z",
     "iopub.status.idle": "2021-09-02T17:04:42.493264Z",
     "shell.execute_reply.started": "2021-09-02T17:04:42.475671Z",
     "shell.execute_reply": "2021-09-02T17:04:42.492466Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!mkdir models_456\n! cp ../input/g2net-tf-on-the-fly-cqt-tpu-training-mixup/models_456/*.h5 /kaggle/working/models_456/\n#! cp ../input/g2net-tf-on-the-fly-cqt-tpu-training-mixup/oof/*.csv /kaggle/working/\n! cp ../input/b7-tpu-456-256/models/fold1.h5 /kaggle/working/\n!cp ../input/g2net-tf-on-the-fly-cqt-tpu-training-mixup/models/fold0.h5 /kaggle/working/\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-03T05:20:35.160765Z",
     "iopub.execute_input": "2021-09-03T05:20:35.161178Z",
     "iopub.status.idle": "2021-09-03T05:21:08.604883Z",
     "shell.execute_reply.started": "2021-09-03T05:20:35.161089Z",
     "shell.execute_reply": "2021-09-03T05:21:08.603857Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "resize=True\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(files_train_all)):\n    files_train = files_train_all[trn_idx]\n    files_valid = files_train_all[val_idx]\n    if fold!=0:\n         \n        continue\n    \n    print(\"=\" * 120)\n    print(f\"Fold {fold}\")\n    print(\"=\" * 120)\n\n    train_image_count = count_data_items(files_train)\n    valid_image_count = count_data_items(files_valid)\n\n    tf.keras.backend.clear_session()\n    strategy, tpu_detected = auto_select_accelerator()\n    with strategy.scope():\n        model = build_model(\n            size=IMAGE_SIZE, \n            efficientnet_size=EFFICIENTNET_SIZE,\n            weights=WEIGHTS, \n            count=train_image_count // BATCH_SIZE // REPLICAS // 4)\n    \n    model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n        str(SAVEDIR / f\"fold{fold}.h5\"), monitor=\"val_auc\", verbose=1, save_best_only=True,\n        save_weights_only=True, mode=\"max\", save_freq=\"epoch\"\n    )\n    if resize:\n        load_dir=Path('./models_456')\n        model.load_weights(str(load_dir / f\"fold{fold}.h5\"))\n        EPOCHS=21\n        print('loaded',fold)\n    \n    history = model.fit(\n        get_dataset(files_train, batch_size=BATCH_SIZE, shuffle=True, \n                    repeat=True, aug=True),\n        epochs=EPOCHS,\n        callbacks=[model_ckpt, get_lr_callback(BATCH_SIZE, REPLICAS,resize)],\n        steps_per_epoch=train_image_count // BATCH_SIZE // REPLICAS // 4,\n        validation_data=get_dataset(files_valid, batch_size=BATCH_SIZE * 4,\n                                    repeat=False, shuffle=False, aug=False),\n        verbose=1\n    )\n\n    \n \n        \n\n    ds_valid = get_dataset(files_valid, labeled=False, return_image_ids=False, \n                           repeat=True, shuffle=False, batch_size=BATCH_SIZE * 2, \n                           aug=False,valid=True)\n    STEPS = valid_image_count / BATCH_SIZE / 2 / REPLICAS\n    pred = model.predict(ds_valid, steps=STEPS, verbose=0)[:valid_image_count]\n    \n    oof_pred.append(np.mean(pred.reshape((valid_image_count, 1), order=\"F\"), axis=1))\n\n    \n    ds_valid = get_dataset(files_valid, repeat=False, labeled=True, \n                           return_image_ids=True, aug=False,valid=True)\n    oof_target.append(np.array([target.numpy() for img, target  in iter(ds_valid.unbatch())]))\n    \n    \n    ds_valid = get_dataset(files_valid, repeat=False, labeled=False, \n                           return_image_ids=True, aug=False)\n    oof_img_ids.append(np.array([target.numpy() for img, target in iter(ds_valid.unbatch())]))\n    print(\"Done with fold  ...\",fold)\n    #oof_img_ids.append(np.array([ids.numpy() for _, _,ids in iter(ds_valid.unbatch())]))\n    '''\n    plt.figure(figsize=(8, 6))\n    sns.distplot(oof_pred[-1])\n    plt.show()\n\n    plt.figure(figsize=(15, 5))\n    plt.plot(\n        np.arange(len(history.history[\"auc\"])),\n        history.history[\"auc\"],\n        \"-o\",\n        label=\"Train auc\",\n        color=\"#ff7f0e\")\n    plt.plot(\n        np.arange(len(history.history[\"auc\"])),\n        history.history[\"val_auc\"],\n        \"-o\",\n        label=\"Val auc\",\n        color=\"#1f77b4\")\n    \n    x = np.argmax(history.history[\"val_auc\"])\n    y = np.max(history.history[\"val_auc\"])\n\n    xdist = plt.xlim()[1] - plt.xlim()[0]\n    ydist = plt.ylim()[1] - plt.ylim()[0]\n\n    plt.scatter(x, y, s=200, color=\"#1f77b4\")\n    plt.text(x - 0.03 * xdist, y - 0.13 * ydist, f\"max auc\\n{y}\", size=14)\n\n    plt.ylabel(\"auc\", size=14)\n    plt.xlabel(\"Epoch\", size=14)\n    plt.legend(loc=2)\n\n    plt2 = plt.gca().twinx()\n    plt2.plot(\n        np.arange(len(history.history[\"auc\"])),\n        history.history[\"loss\"],\n        \"-o\",\n        label=\"Train Loss\",\n        color=\"#2ca02c\")\n    plt2.plot(\n        np.arange(len(history.history[\"auc\"])),\n        history.history[\"val_loss\"],\n        \"-o\",\n        label=\"Val Loss\",\n        color=\"#d62728\")\n    \n    x = np.argmin(history.history[\"val_loss\"])\n    y = np.min(history.history[\"val_loss\"])\n    \n    ydist = plt.ylim()[1] - plt.ylim()[0]\n\n    plt.scatter(x, y, s=200, color=\"#d62728\")\n    plt.text(x - 0.03 * xdist, y + 0.05 * ydist, \"min loss\", size=14)\n\n    plt.ylabel(\"Loss\", size=14)\n    plt.title(f\"Fold {fold + 1} - Image Size {IMAGE_SIZE}, EfficientNetB{EFFICIENTNET_SIZE}\", size=18)\n\n    plt.legend(loc=3)\n    plt.savefig(OOFDIR / f\"fig{fold}.png\")\n    plt.show()\n    '''",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T17:04:42.494343Z",
     "iopub.execute_input": "2021-09-02T17:04:42.494777Z",
     "iopub.status.idle": "2021-09-02T17:06:09.485972Z",
     "shell.execute_reply.started": "2021-09-02T17:04:42.494747Z",
     "shell.execute_reply": "2021-09-02T17:06:09.479681Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#!mkdir models\n#!cp ../input/b7-tpu-456-256/models/fold0.h5 /kaggle/working/models/\n#!cp ../input/g2net-tf-on-the-fly-cqt-tpu-training-mixup/models/fold2.h5 /kaggle/working/models/\n#!cp ../input/g2net-tf-on-the-fly-cqt-tpu-training-mixup/models/fold3.h5 /kaggle/working/models/",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!ls -l models",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-08-28T04:19:30.16913Z",
     "iopub.execute_input": "2021-08-28T04:19:30.169668Z",
     "iopub.status.idle": "2021-08-28T04:19:30.916132Z",
     "shell.execute_reply.started": "2021-08-28T04:19:30.169633Z",
     "shell.execute_reply": "2021-08-28T04:19:30.914845Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## OOF",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "oof = np.concatenate(oof_pred)\ntrue = np.concatenate(oof_target)\nauc = roc_auc_score(y_true=true, y_score=oof)\nprint(f\"AUC: {auc:.5f}\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "try:\n    \n    img_ids = np.concatenate(oof_img_ids)\n\n    df = pd.DataFrame({\n        'img_ids':img_ids,\n        \"y_true\": true.reshape(-1),\n        \"y_pred\": oof\n    })\n    df.head()\n    df.to_csv(OOFDIR / f\"oof_512_b7_fold{fold}.csv\", index=False)\nexcept:\n    None \n    print('None')\n    df.to_csv(OOFDIR / f\"oof_512_b7_fold3.csv\", index=False)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#df.to_csv('oof_b7_more_img_sz_1.csv',index=False)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}