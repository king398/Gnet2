{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "g2net_effb7_100_seed_21.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19393.487526,
   "end_time": "2021-09-09T22:40:53.091056",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-09T17:17:39.603530",
   "version": "2.3.3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "e9ffa754"
   },
   "source": [
    "!pip install -q efficientnet\n",
    "!pip install -q tensorflow-addons\n",
    "!pip install -q tf-madgrad\n",
    "!pip install -q tensorflow == 2.5.1\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"Sharpness-Aware-Minimization-TensorFlow\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import get_window\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import efficientnet.tfkeras as efn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow_addons as tfa\n"
   ],
   "id": "e9ffa754",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b2498d1",
    "outputId": "4063474d-c0ab-40be-e808-39afa630c9b4"
   },
   "source": [
    "# Function to get hardware strategy\n",
    "def get_hardware_strategy():\n",
    "\ttry:\n",
    "\t\t# TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "\t\t# set: this is always the case on Kaggle.\n",
    "\t\ttpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "\t\tprint('Running on TPU ', tpu.master())\n",
    "\texcept ValueError:\n",
    "\t\ttpu = None\n",
    "\n",
    "\tif tpu:\n",
    "\t\ttf.config.experimental_connect_to_cluster(tpu)\n",
    "\t\ttf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\t\tstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\t\tpolicy = mixed_precision.Policy('mixed_bfloat16')\n",
    "\t\tmixed_precision.set_global_policy(policy)\n",
    "\t\ttf.config.optimizer.set_jit(True)\n",
    "\telse:\n",
    "\t\t# Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\t\tstrategy = tf.distribute.get_strategy()\n",
    "\n",
    "\tprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "\treturn tpu, strategy\n",
    "\n",
    "\n",
    "tpu, strategy = get_hardware_strategy()"
   ],
   "id": "7b2498d1",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running on TPU  grpc://10.32.150.50:8470\n",
      "INFO:tensorflow:Initializing the TPU system: grpc://10.32.150.50:8470\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.32.150.50:8470\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "REPLICAS:  8\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3ed66213"
   },
   "source": [
    "# For tf.dataset\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "GCS_PATH1 = \"gs://kds-0ce57d316a43123380114b5d7ad7220bd2d98bc2d2ca3c6b12066dcb\"\n",
    "GCS_PATH2 = \"gs://kds-00963e1b0402e5e11824a284b3c7b57cc1aa90a7f848ca0f70ea02c1\"\n",
    "GCS_PATH3 = \"gs://kds-0a1b764bbf29152f53ab0a3271970056596514c6dc3c05f74e5bc3ad\"\n",
    "# Data access (Test tf records)\n",
    "GCS_PATH4 = \"gs://kds-0f37e33fcfe9fece00f03f55c79d28611c576e844d1293ba786136ea\"\n",
    "GCS_PATH5 = \"gs://kds-cc48964f90b8da9c9d79a6c8c500e7aae7fd9d6dc69f243deb29b5c9\"\n",
    "EPOCHS = 18\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "IMAGE_SIZE = [512, 512]\n",
    "# Seed\n",
    "SEED = 1991\n",
    "# Learning rate\n",
    "LR = 0.0001\n",
    "# Verbosity\n",
    "VERBOSE = 1\n",
    "\n",
    "# Training filenames directory\n",
    "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH1 + '/train*.tfrec') + tf.io.gfile.glob(\n",
    "\tGCS_PATH2 + '/train*.tfrec') + tf.io.gfile.glob(GCS_PATH3 + '/train*.tfrec')\n",
    "# Testing filenames directory\n",
    "TESTING_FILENAMES = tf.io.gfile.glob(GCS_PATH4 + '/test*.tfrec') + tf.io.gfile.glob(GCS_PATH5 + '/test*.tfrec')"
   ],
   "id": "3ed66213",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7dc991a",
    "outputId": "09346c33-6ddf-464e-9ecc-a7c3d91b3f8c"
   },
   "source": [
    "\n",
    "# Function to create cqt kernel\n",
    "def create_cqt_kernels(\n",
    "\t\tq: float,\n",
    "\t\tfs: float,\n",
    "\t\tfmin: float,\n",
    "\t\tn_bins: int = 84,\n",
    "\t\tbins_per_octave: int = 12,\n",
    "\t\tnorm: float = 1,\n",
    "\t\twindow: str = \"hann\",\n",
    "\t\tfmax: Optional[float] = None,\n",
    "\t\ttopbin_check: bool = True\n",
    ") -> Tuple[np.ndarray, int, np.ndarray, float]:\n",
    "\tfft_len = 2 ** _nextpow2(np.ceil(q * fs / fmin))\n",
    "\n",
    "\tif (fmax is not None) and (n_bins is None):\n",
    "\t\tn_bins = np.ceil(bins_per_octave * np.log2(fmax / fmin))\n",
    "\t\tfreqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n",
    "\telif (fmax is None) and (n_bins is not None):\n",
    "\t\tfreqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n",
    "\telse:\n",
    "\t\twarnings.warn(\"If nmax is given, n_bins will be ignored\", SyntaxWarning)\n",
    "\t\tn_bins = np.ceil(bins_per_octave * np.log2(fmax / fmin))\n",
    "\t\tfreqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n",
    "\n",
    "\tif np.max(freqs) > fs / 2 and topbin_check:\n",
    "\t\traise ValueError(f\"The top bin {np.max(freqs)} Hz has exceeded the Nyquist frequency, \\\n",
    "                           please reduce the `n_bins`\")\n",
    "\n",
    "\tkernel = np.zeros((int(n_bins), int(fft_len)), dtype=np.complex64)\n",
    "\n",
    "\tlength = np.ceil(q * fs / freqs)\n",
    "\tfor k in range(0, int(n_bins)):\n",
    "\t\tfreq = freqs[k]\n",
    "\t\tl = np.ceil(q * fs / freq)\n",
    "\n",
    "\t\tif l % 2 == 1:\n",
    "\t\t\tstart = int(np.ceil(fft_len / 2.0 - l / 2.0)) - 1\n",
    "\t\telse:\n",
    "\t\t\tstart = int(np.ceil(fft_len / 2.0 - l / 2.0))\n",
    "\n",
    "\t\tsig = get_window(window, int(l), fftbins=True) * np.exp(\n",
    "\t\t\tnp.r_[-l // 2:l // 2] * 1j * 2 * np.pi * freq / fs) / l\n",
    "\n",
    "\t\tif norm:\n",
    "\t\t\tkernel[k, start:start + int(l)] = sig / np.linalg.norm(sig, norm)\n",
    "\t\telse:\n",
    "\t\t\tkernel[k, start:start + int(l)] = sig\n",
    "\treturn kernel, fft_len, length, freqs\n",
    "\n",
    "\n",
    "class BasicConv(tf.keras.Model):\n",
    "\tdef __init__(self, out_planes, kernel_size, stride=1, padding=\"same\", dilation=1, groups=1, relu=True,\n",
    "\t             bn=True, bias=False):\n",
    "\t\tsuper(BasicConv, self).__init__()\n",
    "\n",
    "\t\tself.out_channels = out_planes\n",
    "\t\tself.conv = tf.keras.layers.Conv2D(filters=out_planes, kernel_size=kernel_size, strides=stride, padding=padding,\n",
    "\t\t                                   dilation_rate=dilation, groups=groups, use_bias=bias)\n",
    "\t\tself.bn = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.01, fused=True) if bn else None\n",
    "\t\tself.relu = tf.keras.layers.ReLU() if relu else None\n",
    "\n",
    "\tdef call(self, x):\n",
    "\t\tx = self.conv(x)\n",
    "\t\tif self.bn is not None:\n",
    "\t\t\tx = self.bn(x)\n",
    "\t\tif self.relu is not None:\n",
    "\t\t\tx = self.relu(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class ChannelPool(tf.keras.Model):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(ChannelPool, self).__init__()\n",
    "\n",
    "\tdef __call__(self, x):\n",
    "\t\treturn tf.reduce_max(x, axis=[3], keepdims=True)\n",
    "\n",
    "\n",
    "class SpatialGate(tf.keras.Model):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(SpatialGate, self).__init__()\n",
    "\t\tkernel_size = 7\n",
    "\t\tself.compress = ChannelPool()\n",
    "\t\tself.spatial = BasicConv(out_planes=1, kernel_size=kernel_size, stride=1, relu=False)\n",
    "\n",
    "\tdef __call__(self, x):\n",
    "\t\tx_compress = self.compress(x)\n",
    "\t\tx_out = self.spatial(x_compress)\n",
    "\t\tscale = tf.math.sigmoid(x_out)\n",
    "\t\treturn x * scale\n",
    "\n",
    "\n",
    "class TripletAttention(tf.keras.Model):\n",
    "\tdef __init__(self, no_spatial=False):\n",
    "\t\tsuper(TripletAttention, self).__init__()\n",
    "\t\tself.ChannelGateH = SpatialGate()\n",
    "\t\tself.ChannelGateW = SpatialGate()\n",
    "\t\tself.no_spatial = no_spatial\n",
    "\t\tif not no_spatial:\n",
    "\t\t\tself.SpatialGate = SpatialGate()\n",
    "\n",
    "\tdef call(self, x):\n",
    "\t\tx_perm1 = tf.transpose(x, perm=(0, 2, 1, 3))\n",
    "\t\tx_out1 = self.ChannelGateH(x_perm1)\n",
    "\t\tx_out11 = tf.transpose(x_out1, (0, 2, 1, 3))\n",
    "\t\tx_perm2 = tf.transpose(x, perm=(0, 3, 2, 1))\n",
    "\t\tx_out2 = self.ChannelGateW(x_perm2)\n",
    "\t\tx_out21 = tf.transpose(x_out2, (0, 3, 2, 1))\n",
    "\t\tif not self.no_spatial:\n",
    "\t\t\tx_out = self.SpatialGate(x)\n",
    "\t\t\tx_out = (1 / 3) * (x_out + x_out11 + x_out21)\n",
    "\t\telse:\n",
    "\t\t\tx_out = (1 / 2) * (x_out11 + x_out21)\n",
    "\t\treturn x_out\n",
    "\n",
    "\n",
    "model_aug = TripletAttention()\n",
    "\n",
    "\n",
    "def _nextpow2(a: float) -> int:\n",
    "\treturn int(np.ceil(np.log2(a)))\n",
    "\n",
    "\n",
    "# Function to prepare cqt kernel\n",
    "def prepare_cqt_kernel(\n",
    "\t\tsr=22050,\n",
    "\t\thop_length=512,\n",
    "\t\tfmin=32.70,\n",
    "\t\tfmax=None,\n",
    "\t\tn_bins=84,\n",
    "\t\tbins_per_octave=12,\n",
    "\t\tnorm=1,\n",
    "\t\tfilter_scale=1,\n",
    "\t\twindow=\"hann\"\n",
    "):\n",
    "\tq = float(filter_scale) / (2 ** (1 / bins_per_octave) - 1)\n",
    "\tprint(q)\n",
    "\treturn create_cqt_kernels(q, sr, fmin, n_bins, bins_per_octave, norm, window, fmax)\n",
    "\n",
    "\n",
    "# Function to create cqt image\n",
    "def create_cqt_image(wave, hop_length=16):\n",
    "\tCQTs = []\n",
    "\tfor i in range(3):\n",
    "\t\tx = wave[i]\n",
    "\t\tx = tf.expand_dims(tf.expand_dims(x, 0), 2)\n",
    "\t\tx = tf.pad(x, PADDING, \"REFLECT\")\n",
    "\n",
    "\t\tCQT_real = tf.nn.conv1d(x, CQT_KERNELS_REAL, stride=hop_length, padding=\"VALID\")\n",
    "\t\tCQT_imag = -tf.nn.conv1d(x, CQT_KERNELS_IMAG, stride=hop_length, padding=\"VALID\")\n",
    "\t\tCQT_real *= tf.math.sqrt(LENGTHS)\n",
    "\t\tCQT_imag *= tf.math.sqrt(LENGTHS)\n",
    "\n",
    "\t\tCQT = tf.math.sqrt(tf.pow(CQT_real, 2) + tf.pow(CQT_imag, 2))\n",
    "\t\tCQTs.append(CQT[0])\n",
    "\treturn tf.stack(CQTs, axis=2)\n",
    "\n",
    "\n",
    "HOP_LENGTH = 6\n",
    "cqt_kernels, KERNEL_WIDTH, lengths, _ = prepare_cqt_kernel(\n",
    "\tsr=2048,\n",
    "\thop_length=HOP_LENGTH,\n",
    "\tfmin=20,\n",
    "\tfmax=1024,\n",
    "\tbins_per_octave=9)\n",
    "LENGTHS = tf.constant(lengths, dtype=tf.float32)\n",
    "CQT_KERNELS_REAL = tf.constant(np.swapaxes(cqt_kernels.real[:, np.newaxis, :], 0, 2))\n",
    "CQT_KERNELS_IMAG = tf.constant(np.swapaxes(cqt_kernels.imag[:, np.newaxis, :], 0, 2))\n",
    "PADDING = tf.constant([[0, 0],\n",
    "                       [KERNEL_WIDTH // 2, KERNEL_WIDTH // 2],\n",
    "                       [0, 0]])\n"
   ],
   "id": "b7dc991a",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12.490672763062207\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: SyntaxWarning: If nmax is given, n_bins will be ignored\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb44cdfa",
    "outputId": "1fbc8c6d-c8a7-4b8b-fcf9-bca742983197"
   },
   "source": [
    "\n",
    "class GeMPoolingLayer(tf.keras.layers.Layer):\n",
    "\tdef __init__(self, p=1., train_p=False):\n",
    "\t\tsuper().__init__()\n",
    "\t\tif train_p:\n",
    "\t\t\tself.p = tf.Variable(p, dtype=tf.float32)\n",
    "\t\telse:\n",
    "\t\t\tself.p = p\n",
    "\t\tself.eps = 1e-6\n",
    "\n",
    "\tdef call(self, inputs: tf.Tensor, **kwargs):\n",
    "\t\tinputs = tf.clip_by_value(inputs, clip_value_min=1e-6, clip_value_max=tf.reduce_max(inputs))\n",
    "\t\tinputs = tf.pow(inputs, self.p)\n",
    "\t\tinputs = tf.reduce_mean(inputs, axis=[1, 2], keepdims=False)\n",
    "\t\tinputs = tf.pow(inputs, 1. / self.p)\n",
    "\t\treturn inputs\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "\trandom.seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\ttf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "# Function to prepare image\n",
    "def prepare_image(wave):\n",
    "\t# Decode raw\n",
    "\twave = tf.reshape(tf.io.decode_raw(wave, tf.float64), (3, 4096))\n",
    "\tscaling = tf.constant([1.5e-20, 1.5e-20, 0.5e-20], dtype=tf.float64)\n",
    "\n",
    "\tnormalized_waves = []\n",
    "\t# Normalize\n",
    "\tfor i in range(3):\n",
    "\t\tnormalized_wave = wave[i] / scaling[i]\n",
    "\t\tnormalized_waves.append(normalized_wave)\n",
    "\t# Stack and cast\n",
    "\twave = tf.stack(normalized_waves)\n",
    "\twave = tf.cast(wave, tf.float32)\n",
    "\t# Create image\n",
    "\timage = create_cqt_image(wave, HOP_LENGTH)\n",
    "\t# Resize image\n",
    "\timage = tf.image.resize(image, [*IMAGE_SIZE])\n",
    "\n",
    "\t# Reshape\n",
    "\timage = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "\treturn tf.cast(image, tf.bfloat16)\n",
    "\n",
    "\n",
    "# This function parse our images and also get the target variable\n",
    "def read_labeled_tfrecord(example):\n",
    "\tLABELED_TFREC_FORMAT = {\n",
    "\t\t'wave': tf.io.FixedLenFeature([], tf.string),\n",
    "\t\t'wave_id': tf.io.FixedLenFeature([], tf.string),\n",
    "\t\t'target': tf.io.FixedLenFeature([], tf.int64)\n",
    "\t}\n",
    "\texample = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "\timage = prepare_image(example['wave'])\n",
    "\timage_id = example['wave_id']\n",
    "\ttarget = tf.cast(example['target'], tf.float32)\n",
    "\treturn image, image_id, target\n",
    "\n",
    "\n",
    "# This function parse our images and also get the target variable\n",
    "def read_unlabeled_tfrecord(example):\n",
    "\tLABELED_TFREC_FORMAT = {\n",
    "\t\t'wave': tf.io.FixedLenFeature([], tf.string),\n",
    "\t\t'wave_id': tf.io.FixedLenFeature([], tf.string)\n",
    "\t}\n",
    "\texample = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "\timage = prepare_image(example['wave'])\n",
    "\timage_id = example['wave_id']\n",
    "\treturn image, image_id\n",
    "\n",
    "\n",
    "# This function loads TF Records and parse them into tensors\n",
    "def load_dataset(filenames, ordered=False, labeled=True):\n",
    "\tignore_order = tf.data.Options()\n",
    "\tif not ordered:\n",
    "\t\tignore_order.experimental_deterministic = False\n",
    "\n",
    "\tdataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "\tdataset = dataset.with_options(ignore_order)\n",
    "\tdataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "# This function is to get our training dataset\n",
    "def get_training_dataset(filenames, ordered=False, labeled=True):\n",
    "\tdataset = load_dataset(filenames, ordered=ordered, labeled=labeled)\n",
    "\tdataset = dataset.repeat()\n",
    "\tdataset = dataset.shuffle(2048)\n",
    "\tdataset = dataset.batch(BATCH_SIZE)\n",
    "\tdataset = dataset.prefetch(AUTO)\n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "# This function is to get our validation and test dataset\n",
    "def get_val_test_dataset(filenames, ordered=True, labeled=True):\n",
    "\tdataset = load_dataset(filenames, ordered=ordered, labeled=labeled)\n",
    "\tdataset = dataset.batch(BATCH_SIZE)\n",
    "\tdataset = dataset.prefetch(AUTO)\n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "# Function to count how many photos we have in\n",
    "def count_data_items(filenames):\n",
    "\t# The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "\tn = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "\treturn np.sum(n)\n",
    "\n",
    "\n",
    "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
    "NUM_TESTING_IMAGES = count_data_items(TESTING_FILENAMES)\n",
    "print(f'Dataset: {NUM_TRAINING_IMAGES} training images')\n",
    "print(f'Dataset: {NUM_TESTING_IMAGES} testing images')\n"
   ],
   "id": "cb44cdfa",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset: 560000 training images\n",
      "Dataset: 226000 testing images\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cdb375b",
    "outputId": "a5438005-7129-4644-9cea-672d37edba95"
   },
   "source": [
    "\n",
    "class SAM():\n",
    "\tdef __init__(self, base_optimizer, rho=0.05):\n",
    "\t\tassert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "\t\tself.rho = rho\n",
    "\t\tself.base_optimizer = base_optimizer\n",
    "\n",
    "\tdef first_step(self, gradients, trainable_variables):\n",
    "\t\tself.e_ws = []\n",
    "\t\tgrad_norm = tf.linalg.global_norm(gradients)\n",
    "\t\tfor i in range(len(trainable_variables)):\n",
    "\t\t\te_w = gradients[i] * self.rho / (grad_norm + 1e-12)\n",
    "\t\t\ttrainable_variables[i].assign_add(e_w)\n",
    "\t\t\tself.e_ws.append(e_w)\n",
    "\n",
    "\tdef second_step(self, gradients, trainable_variables):\n",
    "\t\tfor i in range(len(trainable_variables)):\n",
    "\t\t\ttrainable_variables[i].assign_add(-self.e_ws[i])\n",
    "\t\t# do the actual \"sharpness-aware\" update\n",
    "\t\tself.base_optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "\n",
    "# if you want to use model.fit(), override the train_step method of a model with this function, example is mnist_example_keras_fit.\n",
    "# for customization see https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit/\n",
    "def sam_train_step(self, data, rho=0.05, eps=1e-12):\n",
    "\t# Unpack the data. Its structure depends on your model and\n",
    "\t# on what you pass to `fit()`.\n",
    "\tif len(data) == 3:\n",
    "\t\tx, y, sample_weight = data\n",
    "\telse:\n",
    "\t\tsample_weight = None\n",
    "\t\tx, y = data\n",
    "\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\ty_pred = self(x, training=True)  # Forward pass\n",
    "\t\t# Compute the loss value\n",
    "\t\t# (the loss function is configured in `compile()`)\n",
    "\t\tloss = self.compiled_loss(y, y_pred, sample_weight=sample_weight, regularization_losses=self.losses)\n",
    "\n",
    "\t# Compute gradients\n",
    "\ttrainable_vars = self.trainable_variables\n",
    "\tgradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "\t# first step\n",
    "\te_ws = []\n",
    "\tgrad_norm = tf.linalg.global_norm(gradients)\n",
    "\tfor i in range(len(trainable_vars)):\n",
    "\t\te_w = gradients[i] * rho / (grad_norm + eps)\n",
    "\t\ttrainable_vars[i].assign_add(e_w)\n",
    "\t\te_ws.append(e_w)\n",
    "\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\ty_pred = self(x, training=True)  # Forward pass\n",
    "\t\t# Compute the loss value\n",
    "\t\t# (the loss function is configured in `compile()`)\n",
    "\t\tloss = self.compiled_loss(y, y_pred, sample_weight=sample_weight, regularization_losses=self.losses)\n",
    "\n",
    "\ttrainable_vars = self.trainable_variables\n",
    "\tgradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "\tfor i in range(len(trainable_vars)):\n",
    "\t\ttrainable_vars[i].assign_sub(e_ws[i])\n",
    "\tself.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "\t# Update the metrics.\n",
    "\t# Metrics are configured in `compile()`.\n",
    "\tself.compiled_metrics.update_state(y, y_pred, sample_weight=sample_weight)\n",
    "\n",
    "\t# Return a dict mapping metric names to current value.\n",
    "\t# Note that it will include the loss (tracked in self.metrics).\n",
    "\treturn {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "# Learning rate callback function\n",
    "def get_lr_callback():\n",
    "\tlr_start = 0.0001\n",
    "\tlr_max = 0.000015 * BATCH_SIZE\n",
    "\tlr_min = 0.0000001\n",
    "\tlr_ramp_ep = 3\n",
    "\tlr_sus_ep = 0\n",
    "\tlr_decay = 0.7\n",
    "\n",
    "\tdef lrfn(epoch):\n",
    "\t\tif epoch < lr_ramp_ep:\n",
    "\t\t\tlr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "\t\telif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "\t\t\tlr = lr_max\n",
    "\t\telse:\n",
    "\t\t\tlr = (lr_max - lr_min) * lr_decay ** (epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "\t\treturn lr\n",
    "\n",
    "\tlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=VERBOSE)\n",
    "\treturn lr_callback\n",
    "\n",
    "\n",
    "import math\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class CosineAnnealingScheduler(Callback):\n",
    "\t\"\"\"Cosine annealing scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "\tdef __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
    "\t\tsuper(CosineAnnealingScheduler, self).__init__()\n",
    "\t\tself.T_max = T_max\n",
    "\t\tself.eta_max = eta_max\n",
    "\t\tself.eta_min = eta_min\n",
    "\t\tself.verbose = verbose\n",
    "\n",
    "\tdef on_epoch_begin(self, epoch, logs=None):\n",
    "\t\tif not hasattr(self.model.optimizer, 'lr'):\n",
    "\t\t\traise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "\t\tlr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
    "\t\tK.set_value(self.model.optimizer.lr, lr)\n",
    "\t\tif self.verbose > 0:\n",
    "\t\t\tprint('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
    "\t\t\t      'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "\tdef on_epoch_end(self, epoch, logs=None):\n",
    "\t\tlogs = logs or {}\n",
    "\t\tlogs['lr'] = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "\n",
    "decay = CosineAnnealingScheduler(T_max=100, eta_max=1e-3, eta_min=1e-6)\n",
    "\n",
    "\n",
    "# Function to create our EfficientNetB7 model\n",
    "def get_model():\n",
    "\twith strategy.scope():\n",
    "\t\tinp = tf.keras.layers.Input(shape=(*IMAGE_SIZE, 3))\n",
    "\t\tx = efn.EfficientNetB7(include_top=False, weights='noisy-student')(inp)\n",
    "\t\tx = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\t\toutput = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\t\tmodel = tf.keras.models.Model(inputs=[inp], outputs=[output])\n",
    "\t\topt = tfa.optimizers.AdamW(learning_rate=LR, weight_decay=decay)\n",
    "\t\topt = tfa.optimizers.SWA(opt)\n",
    "\t\tmodel.compile(\n",
    "\t\t\toptimizer=opt,\n",
    "\t\t\tloss=[tf.keras.losses.BinaryCrossentropy()],\n",
    "\t\t\tmetrics=[tf.keras.metrics.AUC()]\n",
    "\t\t)\n",
    "\t\treturn model\n",
    "\n",
    "\n",
    "options = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "\tfilepath=\"Temp.h5\",\n",
    "\tsave_weights_only=True,\n",
    "\tmonitor='auc',\n",
    "\tmode='max',\n",
    "\tsave_best_only=True, options=options)\n",
    "\n",
    "\n",
    "# Function to train a model with 100% of the data\n",
    "def train_and_evaluate(SEED=42):\n",
    "\tprint('\\n')\n",
    "\tprint('-' * 50)\n",
    "\tprint(f'Training EFFB7 with 100% of the data with seed {SEED} for {EPOCHS} epochs')\n",
    "\tif tpu:\n",
    "\t\ttf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\ttrain_dataset = get_training_dataset(TRAINING_FILENAMES, ordered=False, labeled=True)\n",
    "\ttrain_dataset = train_dataset.map(lambda image, image_id, target: (image, target))\n",
    "\tSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // (BATCH_SIZE * 4)\n",
    "\tK.clear_session()\n",
    "\t# Seed everything\n",
    "\tseed_everything(SEED)\n",
    "\tmodel = get_model()\n",
    "\thistory = model.fit(train_dataset,\n",
    "\t                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "\t                    epochs=EPOCHS,\n",
    "\t                    callbacks=[get_lr_callback(), model_checkpoint_callback],\n",
    "\t                    verbose=1)\n",
    "\n",
    "\tprint('\\n')\n",
    "\tprint('-' * 50)\n",
    "\tprint('Test inference...')\n",
    "\tmodel = model.load_weights(\"content/Temp\", options=options)\n",
    "\t# Predict the test set\n",
    "\tdataset = get_val_test_dataset(TESTING_FILENAMES, ordered=True, labeled=False)\n",
    "\timage = dataset.map(lambda image, image_id: image)\n",
    "\ttest_predictions = model.predict(image).astype(np.float32).reshape(-1)\n",
    "\t# Get the test set image_id\n",
    "\timage_id = dataset.map(lambda image, image_id: image_id).unbatch()\n",
    "\timage_id = next(iter(image_id.batch(NUM_TESTING_IMAGES))).numpy().astype('U')\n",
    "\t# Create dataframe output\n",
    "\ttest_df = pd.DataFrame({'id': image_id, 'target': test_predictions})\n",
    "\t# Save test dataframe to disk\n",
    "\ttest_df.to_csv(f'TEST_EfficientNetB7_{IMAGE_SIZE[0]}_{SEED}.csv', index=False)\n",
    "\n",
    "\n",
    "train_and_evaluate(SEED=2020)\n",
    "g"
   ],
   "id": "0cdb375b",
   "execution_count": null,
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training EFFB7 with 100% of the data with seed 2020 for 18 epochs\n",
      "WARNING:tensorflow:TPU system grpc://10.32.150.50:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.32.150.50:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.32.150.50:8470\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.32.150.50:8470\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/18\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "1093/1093 [==============================] - 1752s 1s/step - loss: 0.5232 - auc: 0.7842\n",
      "Epoch 2/18\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0007066666666666667.\n",
      " 106/1093 [=>............................] - ETA: 17:28 - loss: 0.5069 - auc: 0.8017"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Chm0dAstZWRF"
   },
   "source": [
    "import os\n",
    "from tensorflow.python.profiler import profiler_client\n",
    "\n",
    "tpu_profile_service_address = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466')\n",
    "print(profiler_client.monitor(tpu_profile_service_address, 100, 2))"
   ],
   "id": "Chm0dAstZWRF",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vW9o1ME7d7Rk"
   },
   "source": [],
   "id": "vW9o1ME7d7Rk",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ybq-tY3Uainn"
   },
   "source": [
    "model = get_model()\n",
    "model = model.load_weights(\"/content/content/checkpoint\", options=options)"
   ],
   "id": "ybq-tY3Uainn",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o5yQkdu-s_ly"
   },
   "source": [
    "model = model.load_weights(\"/content/content/checkpoint\", options=options)"
   ],
   "id": "o5yQkdu-s_ly",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kDM3mcYHsU4w"
   },
   "source": [
    "dataset = get_val_test_dataset(TESTING_FILENAMES, ordered=True, labeled=False)\n",
    "image = dataset.map(lambda image, image_id: image)\n",
    "test_predictions = model.predict(image).astype(np.float32).reshape(-1)\n",
    "# Get the test set image_id\n",
    "image_id = dataset.map(lambda image, image_id: image_id).unbatch()\n",
    "image_id = next(iter(image_id.batch(NUM_TESTING_IMAGES))).numpy().astype('U')\n",
    "# Create dataframe output\n",
    "test_df = pd.DataFrame({'id': image_id, 'target': test_predictions})\n",
    "# Save test dataframe to disk\n",
    "test_df.to_csv(f'TEST_EfficientNetB7_{IMAGE_SIZE[0]}_{SEED}.csv', index=False)"
   ],
   "id": "kDM3mcYHsU4w",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ce8p7J24oKZq"
   },
   "source": [],
   "id": "Ce8p7J24oKZq",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lqPm1ZyJlKPZ"
   },
   "source": [
    "print(f'TEST_EfficientNetB7_{IMAGE_SIZE[0]}_{SEED}.csv')"
   ],
   "id": "lqPm1ZyJlKPZ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t7xxwwwkoivi"
   },
   "source": [
    "!cp -r \"TEST_EfficientNetB7_512_1991.csv\" \"/content/drive/MyDrive/models\""
   ],
   "id": "t7xxwwwkoivi",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6KZOGP-Mnw1Y"
   },
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "def get_model():\n",
    "\twith strategy.scope():\n",
    "\t\tinp = tf.keras.layers.Input(shape=(*IMAGE_SIZE, 3))\n",
    "\t\tx = efn.EfficientNetB7(include_top=False, weights='noisy-student')(inp)\n",
    "\t\tx = GeM(4)(x)\n",
    "\t\tx = tf.keras.layers.Flatten()(x)\n",
    "\t\toutput = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\t\tmodel = tf.keras.models.Model(inputs=[inp], outputs=[output])\n",
    "\t\topt = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "\t\topt = tfa.optimizers.SWA(opt)\n",
    "\t\tmodel.compile(\n",
    "\t\t\toptimizer=opt,\n",
    "\t\t\tloss=[tf.keras.losses.BinaryCrossentropy()],\n",
    "\t\t\tmetrics=[tf.keras.metrics.AUC()]\n",
    "\t\t)\n",
    "\t\treturn model\n",
    "\n",
    "def prepare_image(wave):\n",
    "\t# Decode raw\n",
    "\twave = tf.convert_to_tensor(np.load(\"../input/g2net-gravitational-wave-detection/train/0/0/0/00000e74ad.npy\"))\n",
    "\tscaling = tf.constant([1.5e-20, 1.5e-20, 0.5e-20], dtype=tf.float64)\n",
    "\n",
    "\tnormalized_waves = []\n",
    "\t# Normalize\n",
    "\tfor i in range(3):\n",
    "\t\tnormalized_wave = wave[i] / scaling[i]\n",
    "\t\tnormalized_waves.append(normalized_wave)\n",
    "\t# Stack and cast\n",
    "\twave = tf.stack(normalized_waves)\n",
    "\twave = tf.cast(wave, tf.float32)\n",
    "\t# Create image\n",
    "\timage = create_cqt_image(wave, HOP_LENGTH)\n",
    "\tprint(image.shape)\n",
    "\n",
    "\t# Resize image\n",
    "\timage = tf.image.resize(image, [*IMAGE_SIZE])\n",
    "\n",
    "\t# Reshape\n",
    "\timage = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "\treturn tf.cast(image, tf.bfloat16)"
   ],
   "id": "6KZOGP-Mnw1Y",
   "execution_count": null,
   "outputs": []
  }
 ]
}