{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "g2net-effb7-100-seed-21.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19393.487526,
   "end_time": "2021-09-09T22:40:53.091056",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-09T17:17:39.603530",
   "version": "2.3.3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9ffa754",
    "outputId": "5cd73f77-bcfe-467b-feee-ab852cd3ebbd"
   },
   "source": [
    "!pip install -q efficientnet\n",
    "!pip install -q tensorflow-addons\n",
    "import re\n",
    "!pip install -q git+https: // github.com // Kevin-McIsaac / cmorlet-tensorflow @ Performance --no-deps\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import get_window\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import efficientnet.tfkeras as efn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow_addons as tfa\n"
   ],
   "id": "e9ffa754",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[K     |████████████████████████████████| 50 kB 4.6 MB/s \n",
      "\u001B[K     |████████████████████████████████| 1.1 MB 5.3 MB/s \n",
      "\u001B[?25h  Building wheel for CWT (setup.py) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b2498d1",
    "outputId": "40b0172e-d256-4a41-8606-9da4c49dd049"
   },
   "source": [
    "# Function to get hardware strategy\n",
    "def get_hardware_strategy():\n",
    "\ttry:\n",
    "\t\t# TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "\t\t# set: this is always the case on Kaggle.\n",
    "\t\ttpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "\t\tprint('Running on TPU ', tpu.master())\n",
    "\texcept ValueError:\n",
    "\t\ttpu = None\n",
    "\n",
    "\tif tpu:\n",
    "\t\ttf.config.experimental_connect_to_cluster(tpu)\n",
    "\t\ttf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\t\tstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\t\tpolicy = mixed_precision.Policy('mixed_bfloat16')\n",
    "\t\tmixed_precision.set_global_policy(policy)\n",
    "\t\ttf.config.optimizer.set_jit(True)\n",
    "\telse:\n",
    "\t\t# Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\t\tstrategy = tf.distribute.get_strategy()\n",
    "\n",
    "\tprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "\treturn tpu, strategy\n",
    "\n",
    "\n",
    "tpu, strategy = get_hardware_strategy()"
   ],
   "id": "7b2498d1",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running on TPU  grpc://10.99.48.74:8470\n",
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.99.48.74:8470\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.99.48.74:8470\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "REPLICAS:  8\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3ed66213"
   },
   "source": [
    "# For tf.dataset\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Data access (Train tf records)\n",
    "GCS_PATH1 = \"gs://kds-be973b4d24ed59842deeff7fc17c65f940acfe025d5bb2eb03261137\"\n",
    "GCS_PATH2 = \"gs://kds-2a81609a438ac9f10d51ab54625a7f21f167acc05702fed3bfa405ab\"\n",
    "GCS_PATH3 = \"gs://kds-9f5ea0ead6e9f5958187b731b5132e52b919b3173b744a0dc7c0c93c\"\n",
    "# Data access (Test tf records)\n",
    "GCS_PATH4 = \"gs://kds-5bd625816de42dc656f7ef6832ceedc0fd66cea6f4b639bf1489f4f0\"\n",
    "GCS_PATH5 = \"gs://kds-42a3dac2586f0971b2a3b30162eed7cea42f2f9c34cbe66bd6c46b23\"\n",
    "\n",
    "EPOCHS = 18\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "IMAGE_SIZE = [512, 512]\n",
    "# Seed\n",
    "SEED = 1991\n",
    "# Learning rate\n",
    "LR = 0.0001\n",
    "# Verbosity\n",
    "VERBOSE = 2\n",
    "\n",
    "# Training filenames directory\n",
    "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH1 + '/train*.tfrec') + tf.io.gfile.glob(\n",
    "\tGCS_PATH2 + '/train*.tfrec') + tf.io.gfile.glob(GCS_PATH3 + '/train*.tfrec')\n",
    "# Testing filenames directory\n",
    "TESTING_FILENAMES = tf.io.gfile.glob(GCS_PATH4 + '/test*.tfrec') + tf.io.gfile.glob(GCS_PATH5 + '/test*.tfrec')"
   ],
   "id": "3ed66213",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7dc991a",
    "outputId": "29ab7102-2dac-4743-9a54-51b2decafe31"
   },
   "source": [
    "# Function to create cqt kernel\n",
    "def create_cqt_kernels(\n",
    "\t\tq: float,\n",
    "\t\tfs: float,\n",
    "\t\tfmin: float,\n",
    "\t\tn_bins: int = 84,\n",
    "\t\tbins_per_octave: int = 12,\n",
    "\t\tnorm: float = 1,\n",
    "\t\twindow: str = \"hann\",\n",
    "\t\tfmax: Optional[float] = None,\n",
    "\t\ttopbin_check: bool = True\n",
    ") -> Tuple[np.ndarray, int, np.ndarray, float]:\n",
    "\tfft_len = 2 ** _nextpow2(np.ceil(q * fs / fmin))\n",
    "\n",
    "\tif (fmax is not None) and (n_bins is None):\n",
    "\t\tn_bins = np.ceil(bins_per_octave * np.log2(fmax / fmin))\n",
    "\t\tfreqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n",
    "\telif (fmax is None) and (n_bins is not None):\n",
    "\t\tfreqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n",
    "\telse:\n",
    "\t\twarnings.warn(\"If nmax is given, n_bins will be ignored\", SyntaxWarning)\n",
    "\t\tn_bins = np.ceil(bins_per_octave * np.log2(fmax / fmin))\n",
    "\t\tfreqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n",
    "\n",
    "\tif np.max(freqs) > fs / 2 and topbin_check:\n",
    "\t\traise ValueError(f\"The top bin {np.max(freqs)} Hz has exceeded the Nyquist frequency, \\\n",
    "                           please reduce the `n_bins`\")\n",
    "\n",
    "\tkernel = np.zeros((int(n_bins), int(fft_len)), dtype=np.complex64)\n",
    "\n",
    "\tlength = np.ceil(q * fs / freqs)\n",
    "\tfor k in range(0, int(n_bins)):\n",
    "\t\tfreq = freqs[k]\n",
    "\t\tl = np.ceil(q * fs / freq)\n",
    "\n",
    "\t\tif l % 2 == 1:\n",
    "\t\t\tstart = int(np.ceil(fft_len / 2.0 - l / 2.0)) - 1\n",
    "\t\telse:\n",
    "\t\t\tstart = int(np.ceil(fft_len / 2.0 - l / 2.0))\n",
    "\n",
    "\t\tsig = get_window(window, int(l), fftbins=True) * np.exp(\n",
    "\t\t\tnp.r_[-l // 2:l // 2] * 1j * 2 * np.pi * freq / fs) / l\n",
    "\n",
    "\t\tif norm:\n",
    "\t\t\tkernel[k, start:start + int(l)] = sig / np.linalg.norm(sig, norm)\n",
    "\t\telse:\n",
    "\t\t\tkernel[k, start:start + int(l)] = sig\n",
    "\treturn kernel, fft_len, length, freqs\n",
    "\n",
    "\n",
    "class BasicConv(tf.keras.Model):\n",
    "\tdef __init__(self, out_planes, kernel_size, stride=1, padding=\"same\", dilation=1, groups=1, relu=True,\n",
    "\t             bn=True, bias=False):\n",
    "\t\tsuper(BasicConv, self).__init__()\n",
    "\n",
    "\t\tself.out_channels = out_planes\n",
    "\t\tself.conv = tf.keras.layers.Conv2D(filters=out_planes, kernel_size=kernel_size, strides=stride, padding=padding,\n",
    "\t\t                                   dilation_rate=dilation, groups=groups, use_bias=bias)\n",
    "\t\tself.bn = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.01, fused=True) if bn else None\n",
    "\t\tself.relu = tf.keras.layers.ReLU() if relu else None\n",
    "\n",
    "\tdef call(self, x):\n",
    "\t\tx = self.conv(x)\n",
    "\t\tif self.bn is not None:\n",
    "\t\t\tx = self.bn(x)\n",
    "\t\tif self.relu is not None:\n",
    "\t\t\tx = self.relu(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class ChannelPool(tf.keras.Model):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(ChannelPool, self).__init__()\n",
    "\n",
    "\tdef __call__(self, x):\n",
    "\t\treturn tf.reduce_max(x, axis=[3], keepdims=True)\n",
    "\n",
    "\n",
    "class SpatialGate(tf.keras.Model):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(SpatialGate, self).__init__()\n",
    "\t\tkernel_size = 7\n",
    "\t\tself.compress = ChannelPool()\n",
    "\t\tself.spatial = BasicConv(out_planes=1, kernel_size=kernel_size, stride=1, relu=False)\n",
    "\n",
    "\tdef __call__(self, x):\n",
    "\t\tx_compress = self.compress(x)\n",
    "\t\tx_out = self.spatial(x_compress)\n",
    "\t\tscale = tf.math.sigmoid(x_out)\n",
    "\t\treturn x * scale\n",
    "\n",
    "\n",
    "class TripletAttention(tf.keras.Model):\n",
    "\tdef __init__(self, no_spatial=False):\n",
    "\t\tsuper(TripletAttention, self).__init__()\n",
    "\t\tself.ChannelGateH = SpatialGate()\n",
    "\t\tself.ChannelGateW = SpatialGate()\n",
    "\t\tself.no_spatial = no_spatial\n",
    "\t\tif not no_spatial:\n",
    "\t\t\tself.SpatialGate = SpatialGate()\n",
    "\n",
    "\tdef call(self, x):\n",
    "\t\tx_perm1 = tf.transpose(x, perm=(0, 2, 1, 3))\n",
    "\t\tx_out1 = self.ChannelGateH(x_perm1)\n",
    "\t\tx_out11 = tf.transpose(x_out1, (0, 2, 1, 3))\n",
    "\t\tx_perm2 = tf.transpose(x, perm=(0, 3, 2, 1))\n",
    "\t\tx_out2 = self.ChannelGateW(x_perm2)\n",
    "\t\tx_out21 = tf.transpose(x_out2, (0, 3, 2, 1))\n",
    "\t\tif not self.no_spatial:\n",
    "\t\t\tx_out = self.SpatialGate(x)\n",
    "\t\t\tx_out = (1 / 3) * (x_out + x_out11 + x_out21)\n",
    "\t\telse:\n",
    "\t\t\tx_out = (1 / 2) * (x_out11 + x_out21)\n",
    "\t\treturn x_out\n",
    "\n",
    "\n",
    "class GeMPoolingLayer(tf.keras.layers.Layer):\n",
    "\tdef __init__(self, p=1., train_p=False):\n",
    "\t\tsuper().__init__()\n",
    "\t\tif train_p:\n",
    "\t\t\tself.p = tf.Variable(p, dtype=tf.float32)\n",
    "\t\telse:\n",
    "\t\t\tself.p = p\n",
    "\t\tself.eps = 1e-6\n",
    "\n",
    "\tdef call(self, inputs: tf.Tensor, **kwargs):\n",
    "\t\tinputs = tf.clip_by_value(inputs, clip_value_min=1e-6, clip_value_max=tf.reduce_max(inputs))\n",
    "\t\tinputs = tf.pow(inputs, self.p)\n",
    "\t\tinputs = tf.reduce_mean(inputs, axis=[1, 2], keepdims=False)\n",
    "\t\tinputs = tf.pow(inputs, 1. / self.p)\n",
    "\t\treturn inputs\n",
    "\n",
    "\n",
    "model_aug = TripletAttention()\n",
    "\n",
    "\n",
    "def _nextpow2(a: float) -> int:\n",
    "\treturn int(np.ceil(np.log2(a)))\n",
    "\n",
    "\n",
    "# Function to prepare cqt kernel\n",
    "def prepare_cqt_kernel(\n",
    "\t\tsr=22050,\n",
    "\t\thop_length=512,\n",
    "\t\tfmin=32.70,\n",
    "\t\tfmax=None,\n",
    "\t\tn_bins=84,\n",
    "\t\tbins_per_octave=12,\n",
    "\t\tnorm=1,\n",
    "\t\tfilter_scale=1,\n",
    "\t\twindow=\"hann\"\n",
    "):\n",
    "\tq = float(filter_scale) / (2 ** (1 / bins_per_octave) - 1)\n",
    "\tprint(q)\n",
    "\treturn create_cqt_kernels(q, sr, fmin, n_bins, bins_per_octave, norm, window, fmax)\n",
    "\n",
    "\n",
    "# Function to create cqt image\n",
    "def create_cqt_image(wave, hop_length=16):\n",
    "\tCQTs = []\n",
    "\tfor i in range(3):\n",
    "\t\tx = wave[i]\n",
    "\t\tx = tf.expand_dims(tf.expand_dims(x, 0), 2)\n",
    "\t\tx = tf.pad(x, PADDING, \"REFLECT\")\n",
    "\n",
    "\t\tCQT_real = tf.nn.conv1d(x, CQT_KERNELS_REAL, stride=hop_length, padding=\"VALID\")\n",
    "\t\tCQT_imag = -tf.nn.conv1d(x, CQT_KERNELS_IMAG, stride=hop_length, padding=\"VALID\")\n",
    "\t\tCQT_real *= tf.math.sqrt(LENGTHS)\n",
    "\t\tCQT_imag *= tf.math.sqrt(LENGTHS)\n",
    "\n",
    "\t\tCQT = tf.math.sqrt(tf.pow(CQT_real, 2) + tf.pow(CQT_imag, 2))\n",
    "\t\tCQTs.append(CQT[0])\n",
    "\treturn tf.stack(CQTs, axis=2)\n",
    "\n",
    "\n",
    "HOP_LENGTH = 6\n",
    "cqt_kernels, KERNEL_WIDTH, lengths, _ = prepare_cqt_kernel(\n",
    "\tsr=2048,\n",
    "\thop_length=HOP_LENGTH,\n",
    "\tfmin=20,\n",
    "\tfmax=1024,\n",
    "\tbins_per_octave=9)\n",
    "LENGTHS = tf.constant(lengths, dtype=tf.float32)\n",
    "CQT_KERNELS_REAL = tf.constant(np.swapaxes(cqt_kernels.real[:, np.newaxis, :], 0, 2))\n",
    "CQT_KERNELS_IMAG = tf.constant(np.swapaxes(cqt_kernels.imag[:, np.newaxis, :], 0, 2))\n",
    "PADDING = tf.constant([[0, 0],\n",
    "                       [KERNEL_WIDTH // 2, KERNEL_WIDTH // 2],\n",
    "                       [0, 0]])"
   ],
   "id": "b7dc991a",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12.490672763062207\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: SyntaxWarning: If nmax is given, n_bins will be ignored\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb44cdfa",
    "outputId": "c315e8c1-9382-4d10-82e8-695c2fd65226"
   },
   "source": [
    "def seed_everything(seed):\n",
    "\trandom.seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\ttf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "# Function to prepare image\n",
    "# Function to prepare image\n",
    "def prepare_image(wave):\n",
    "\t# Decode raw\n",
    "\twave = tf.reshape(tf.io.decode_raw(wave, tf.float64), (3, 4096))\n",
    "\tnormalized_waves = []\n",
    "\t# Normalize\n",
    "\tfor i in range(3):\n",
    "\t\tnormalized_wave = wave[i] / tf.math.reduce_max(wave[i])\n",
    "\t\tnormalized_waves.append(normalized_wave)\n",
    "\t# Stack and cast\n",
    "\twave = tf.stack(normalized_waves)\n",
    "\twave = tf.cast(wave, tf.float32)\n",
    "\t# Create image\n",
    "\timage = create_cqt_image(wave, HOP_LENGTH)\n",
    "\timage = tf.convert_to_tensor(model_aug(tf.expand_dims(image, axis=1)))\n",
    "\t# Resize image\n",
    "\timage = tf.image.resize(image, [*IMAGE_SIZE])\n",
    "\timage = tf.squeeze(tfa.image.random_cutout(image, mask_size=10, constant_values=255))\n",
    "\timage = image[0, :, :, :]\n",
    "\n",
    "\t# Reshape\n",
    "\timage = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "\treturn tf.cast(image, tf.bfloat16)\n",
    "\n",
    "\n",
    "# This function parse our images and also get the target variable\n",
    "def read_labeled_tfrecord(example):\n",
    "\tLABELED_TFREC_FORMAT = {\n",
    "\t\t'wave': tf.io.FixedLenFeature([], tf.string),\n",
    "\t\t'wave_id': tf.io.FixedLenFeature([], tf.string),\n",
    "\t\t'target': tf.io.FixedLenFeature([], tf.int64)\n",
    "\t}\n",
    "\texample = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "\timage = prepare_image(example['wave'])\n",
    "\timage_id = example['wave_id']\n",
    "\ttarget = tf.cast(example['target'], tf.float32)\n",
    "\treturn image, image_id, target\n",
    "\n",
    "\n",
    "# This function parse our images and also get the target variable\n",
    "def read_unlabeled_tfrecord(example):\n",
    "\tLABELED_TFREC_FORMAT = {\n",
    "\t\t'wave': tf.io.FixedLenFeature([], tf.string),\n",
    "\t\t'wave_id': tf.io.FixedLenFeature([], tf.string)\n",
    "\t}\n",
    "\texample = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "\timage = prepare_image(example['wave'])\n",
    "\timage_id = example['wave_id']\n",
    "\treturn image, image_id\n",
    "\n",
    "\n",
    "# This function loads TF Records and parse them into tensors\n",
    "def load_dataset(filenames, ordered=False, labeled=True):\n",
    "\tignore_order = tf.data.Options()\n",
    "\tif not ordered:\n",
    "\t\tignore_order.experimental_deterministic = False\n",
    "\n",
    "\tdataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "\tdataset = dataset.with_options(ignore_order)\n",
    "\tdataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "# This function is to get our training dataset\n",
    "def get_training_dataset(filenames, ordered=False, labeled=True):\n",
    "\tdataset = load_dataset(filenames, ordered=ordered, labeled=labeled)\n",
    "\tdataset = dataset.repeat()\n",
    "\tdataset = dataset.shuffle(2048)\n",
    "\tdataset = dataset.batch(BATCH_SIZE)\n",
    "\tdataset = dataset.prefetch(AUTO)\n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "# This function is to get our validation and test dataset\n",
    "def get_val_test_dataset(filenames, ordered=True, labeled=True):\n",
    "\tdataset = load_dataset(filenames, ordered=ordered, labeled=labeled)\n",
    "\tdataset = dataset.batch(BATCH_SIZE)\n",
    "\tdataset = dataset.prefetch(AUTO)\n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "# Function to count how many photos we have in\n",
    "def count_data_items(filenames):\n",
    "\t# The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "\tn = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "\treturn np.sum(n)\n",
    "\n",
    "\n",
    "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
    "NUM_TESTING_IMAGES = count_data_items(TESTING_FILENAMES)\n",
    "print(f'Dataset: {NUM_TRAINING_IMAGES} training images')\n",
    "print(f'Dataset: {NUM_TESTING_IMAGES} testing images')"
   ],
   "id": "cb44cdfa",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset: 560000 training images\n",
      "Dataset: 226000 testing images\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cdb375b",
    "outputId": "46da8e40-29ac-4762-a6d5-bc3a5ace725b"
   },
   "source": [
    "# Learning rate callback function\n",
    "def get_lr_callback():\n",
    "\tlr_start = 0.0001\n",
    "\tlr_max = 0.000015 * BATCH_SIZE\n",
    "\tlr_min = 0.0000001\n",
    "\tlr_ramp_ep = 3\n",
    "\tlr_sus_ep = 0\n",
    "\tlr_decay = 0.7\n",
    "\n",
    "\tdef lrfn(epoch):\n",
    "\t\tif epoch < lr_ramp_ep:\n",
    "\t\t\tlr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "\t\telif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "\t\t\tlr = lr_max\n",
    "\t\telse:\n",
    "\t\t\tlr = (lr_max - lr_min) * lr_decay ** (epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "\t\treturn lr\n",
    "\n",
    "\tlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=VERBOSE)\n",
    "\treturn lr_callback\n",
    "\n",
    "\n",
    "# Function to create our EfficientNetB7 model\n",
    "def get_model():\n",
    "\twith strategy.scope():\n",
    "\t\tinp = tf.keras.layers.Input(shape=(*IMAGE_SIZE, 3))\n",
    "\t\tx = efn.EfficientNetB7(include_top=False, weights='imagenet')(inp)\n",
    "\t\tx = GeMPoolingLayer(p=None,)(x)\n",
    "\t\toutput = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\t\tmodel = tf.keras.models.Model(inputs=[inp], outputs=[output])\n",
    "\t\topt = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "\t\topt = tfa.optimizers.SWA(opt)\n",
    "\t\tmodel.compile(\n",
    "\t\t\toptimizer=opt,\n",
    "\t\t\tloss=[tf.keras.losses.BinaryCrossentropy()],\n",
    "\t\t\tmetrics=[tf.keras.metrics.AUC()]\n",
    "\t\t)\n",
    "\t\treturn model\n",
    "\n",
    "\n",
    "options = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "\tfilepath=\"content/Temp\",\n",
    "\tsave_weights_only=True,\n",
    "\tmonitor='auc',\n",
    "\tmode='max',\n",
    "\tsave_best_only=True, options=options)\n",
    "\n",
    "\n",
    "# Function to train a model with 100% of the data\n",
    "def train_and_evaluate(SEED=42):\n",
    "\tprint('\\n')\n",
    "\tprint('-' * 50)\n",
    "\tprint(f'Training EFFB7 with 100% of the data with seed {SEED} for {EPOCHS} epochs')\n",
    "\tif tpu:\n",
    "\t\ttf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\ttrain_dataset = get_training_dataset(TRAINING_FILENAMES, ordered=False, labeled=True)\n",
    "\ttrain_dataset = train_dataset.map(lambda image, image_id, target: (image, target))\n",
    "\tSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // (BATCH_SIZE * 4)\n",
    "\tK.clear_session()\n",
    "\t# Seed everything\n",
    "\tseed_everything(SEED)\n",
    "\tmodel = get_model()\n",
    "\thistory = model.fit(train_dataset,\n",
    "\t                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "\t                    epochs=EPOCHS,\n",
    "\t                    callbacks=[get_lr_callback(), model_checkpoint_callback],\n",
    "\t                    verbose=1)\n",
    "\n",
    "\tprint('\\n')\n",
    "\tprint('-' * 50)\n",
    "\tprint('Test inference...')\n",
    "\tmodel = model.load_weights(\"content/Temp\", options=options)\n",
    "\t# Predict the test set\n",
    "\tdataset = get_val_test_dataset(TESTING_FILENAMES, ordered=True, labeled=False)\n",
    "\timage = dataset.map(lambda image, image_id: image)\n",
    "\ttest_predictions = model.predict(image).astype(np.float32).reshape(-1)\n",
    "\t# Get the test set image_id\n",
    "\timage_id = dataset.map(lambda image, image_id: image_id).unbatch()\n",
    "\timage_id = next(iter(image_id.batch(NUM_TESTING_IMAGES))).numpy().astype('U')\n",
    "\t# Create dataframe output\n",
    "\ttest_df = pd.DataFrame({'id': image_id, 'target': test_predictions})\n",
    "\t# Save test dataframe to disk\n",
    "\ttest_df.to_csv(f'TEST_EfficientNetB7_{IMAGE_SIZE[0]}_{SEED}.csv', index=False)\n",
    "\n",
    "\n",
    "train_and_evaluate(SEED=2020)\n",
    "g"
   ],
   "id": "0cdb375b",
   "execution_count": null,
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training EFFB7 with 100% of the data with seed 2020 for 18 epochs\n",
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.99.48.74:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.99.48.74:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.99.48.74:8470\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.99.48.74:8470\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/18\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1093/1093 [==============================] - 1695s 1s/step - loss: 0.4984 - auc: 0.8098\n",
      "Epoch 2/18\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0007066666666666667.\n",
      "1093/1093 [==============================] - 1152s 1s/step - loss: 0.4601 - auc: 0.8354\n",
      "Epoch 3/18\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0013133333333333335.\n",
      "1093/1093 [==============================] - 1153s 1s/step - loss: 0.4474 - auc: 0.8448\n",
      "Epoch 4/18\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.00192.\n",
      "1093/1093 [==============================] - 1152s 1s/step - loss: 0.4414 - auc: 0.8496\n",
      "Epoch 5/18\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.00134403.\n",
      "1093/1093 [==============================] - 1152s 1s/step - loss: 0.4303 - auc: 0.8573\n",
      "Epoch 6/18\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.0009408509999999998.\n",
      "1093/1093 [==============================] - 1152s 1s/step - loss: 0.4236 - auc: 0.8611\n",
      "Epoch 7/18\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.0006586256999999998.\n",
      "1093/1093 [==============================] - 1151s 1s/step - loss: 0.4168 - auc: 0.8662\n",
      "Epoch 8/18\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.0004610679899999999.\n",
      "1093/1093 [==============================] - 1155s 1s/step - loss: 0.4099 - auc: 0.8699\n",
      "Epoch 9/18\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.0003227775929999999.\n",
      "1093/1093 [==============================] - 1154s 1s/step - loss: 0.4091 - auc: 0.8711\n",
      "Epoch 10/18\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.00022597431509999993.\n",
      "1093/1093 [==============================] - 1155s 1s/step - loss: 0.4067 - auc: 0.8721\n",
      "Epoch 11/18\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.00015821202056999994.\n",
      "1093/1093 [==============================] - 1155s 1s/step - loss: 0.4034 - auc: 0.8740\n",
      "Epoch 12/18\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.00011077841439899995.\n",
      "1093/1093 [==============================] - 1155s 1s/step - loss: 0.3982 - auc: 0.8770\n",
      "Epoch 13/18\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 7.757489007929996e-05.\n",
      "  59/1093 [>.............................] - ETA: 18:13 - loss: 0.3989 - auc: 0.8773"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ybq-tY3Uainn"
   },
   "source": [],
   "id": "ybq-tY3Uainn",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ce8p7J24oKZq"
   },
   "source": [],
   "id": "Ce8p7J24oKZq",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lqPm1ZyJlKPZ"
   },
   "source": [
    "print(f'TEST_EfficientNetB7_{IMAGE_SIZE[0]}_{SEED}.csv')"
   ],
   "id": "lqPm1ZyJlKPZ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t7xxwwwkoivi"
   },
   "source": [
    "!cp -r \"TEST_EfficientNetB7_512_1991.csv\" \"/content/drive/MyDrive/models\""
   ],
   "id": "t7xxwwwkoivi",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6KZOGP-Mnw1Y"
   },
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "model = model.load_weights(\"/content/content/Temp\", options=options)\n",
    "# Predict the test set\n",
    "dataset = get_val_test_dataset(TESTING_FILENAMES, ordered=True, labeled=False)\n",
    "image = dataset.map(lambda image, image_id: image)\n",
    "test_predictions = model.predict(image).astype(np.float32).reshape(-1)\n",
    "# Get the test set image_id\n",
    "image_id = dataset.map(lambda image, image_id: image_id).unbatch()\n",
    "image_id = next(iter(image_id.batch(NUM_TESTING_IMAGES))).numpy().astype('U')\n",
    "# Create dataframe output\n",
    "test_df = pd.DataFrame({'id': image_id, 'target': test_predictions})\n",
    "# Save test dataframe to disk\n",
    "test_df.to_csv(f'TEST_EfficientNetB7_{IMAGE_SIZE[0]}_{SEED}.csv', index=False)"
   ],
   "id": "6KZOGP-Mnw1Y",
   "execution_count": null,
   "outputs": []
  }
 ]
}