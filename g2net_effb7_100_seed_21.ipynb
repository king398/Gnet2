{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "g2net_effb7_100_seed_21.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19393.487526,
   "end_time": "2021-09-09T22:40:53.091056",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-09T17:17:39.603530",
   "version": "2.3.3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "5attLw_m1BOY"
   },
   "source": [],
   "id": "5attLw_m1BOY",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e9ffa754",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "45ee2368-e197-48d7-f4f1-3b352adc6b24"
   },
   "source": [
    "!pip install -q efficientnet\n",
    "!pip install -q tensorflow-addons\n",
    "!pip install -q tf-madgrad\n",
    "!pip install -q git+https: // github.com // Kevin-McIsaac / cmorlet-tensorflow @ Performance --no-deps\n",
    "!pip install keras == \"2.4.0\"\n",
    "\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"Sharpness-Aware-Minimization-TensorFlow\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import get_window\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import efficientnet.tfkeras as efn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow_addons as tfa\n"
   ],
   "id": "e9ffa754",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: keras==2.4.0 in /usr/local/lib/python3.7/dist-packages (2.4.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (1.4.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (1.19.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (3.13)\n",
      "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (2.5.1)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.6.3)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.34.1)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.37.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.7.4.3)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.5.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.1.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.2.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.6.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.12.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.12)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.17.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.3.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.1.0)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.4.0) (1.5.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (2.23.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (1.35.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (57.4.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (3.3.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (4.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (3.5.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b2498d1",
    "outputId": "959f96db-2c4d-41e4-8c34-897f729549b4"
   },
   "source": [
    "# Function to get hardware strategy\n",
    "def get_hardware_strategy():\n",
    "\ttry:\n",
    "\t\t# TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "\t\t# set: this is always the case on Kaggle.\n",
    "\t\ttpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "\t\tprint('Running on TPU ', tpu.master())\n",
    "\texcept ValueError:\n",
    "\t\ttpu = None\n",
    "\n",
    "\tif tpu:\n",
    "\t\ttf.config.experimental_connect_to_cluster(tpu)\n",
    "\t\ttf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\t\tstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "\t\ttf.config.optimizer.set_jit(True)\n",
    "\telse:\n",
    "\t\t# Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\t\tstrategy = tf.distribute.get_strategy()\n",
    "\n",
    "\tprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "\treturn tpu, strategy\n",
    "\n",
    "\n",
    "tpu, strategy = get_hardware_strategy()"
   ],
   "id": "7b2498d1",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running on TPU  grpc://10.13.177.82:8470\n",
      "INFO:tensorflow:Initializing the TPU system: grpc://10.13.177.82:8470\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.13.177.82:8470\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "REPLICAS:  8\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3ed66213"
   },
   "source": [
    "# For tf.dataset\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "GCS_PATH1 = \"gs://kds-0ce57d316a43123380114b5d7ad7220bd2d98bc2d2ca3c6b12066dcb\"\n",
    "GCS_PATH2 = \"gs://kds-00963e1b0402e5e11824a284b3c7b57cc1aa90a7f848ca0f70ea02c1\"\n",
    "GCS_PATH3 = \"gs://kds-0a1b764bbf29152f53ab0a3271970056596514c6dc3c05f74e5bc3ad\"\n",
    "# Data access (Test tf records)\n",
    "GCS_PATH4 = \"gs://kds-0f37e33fcfe9fece00f03f55c79d28611c576e844d1293ba786136ea\"\n",
    "GCS_PATH5 = \"gs://kds-cc48964f90b8da9c9d79a6c8c500e7aae7fd9d6dc69f243deb29b5c9\"\n",
    "\n",
    "EPOCHS = 18\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "IMAGE_SIZE = [512, 512]\n",
    "# Seed\n",
    "SEED = 1991\n",
    "# Learning rate\n",
    "LR = 0.0001\n",
    "# Verbosity\n",
    "VERBOSE = 1\n",
    "\n",
    "# Training filenames directory\n",
    "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH1 + '/train*.tfrec') + tf.io.gfile.glob(\n",
    "\tGCS_PATH2 + '/train*.tfrec') + tf.io.gfile.glob(GCS_PATH3 + '/train*.tfrec')\n",
    "# Testing filenames directory\n",
    "TESTING_FILENAMES = tf.io.gfile.glob(GCS_PATH4 + '/test*.tfrec') + tf.io.gfile.glob(GCS_PATH5 + '/test*.tfrec')"
   ],
   "id": "3ed66213",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7dc991a",
    "outputId": "7c928387-1ea3-4f74-b203-641f0e6e3faf"
   },
   "source": [
    "\n",
    "# Function to create cqt kernel\n",
    "def create_cqt_kernels(\n",
    "\t\tq: float,\n",
    "\t\tfs: float,\n",
    "\t\tfmin: float,\n",
    "\t\tn_bins: int = 84,\n",
    "\t\tbins_per_octave: int = 12,\n",
    "\t\tnorm: float = 1,\n",
    "\t\twindow: str = \"hann\",\n",
    "\t\tfmax: Optional[float] = None,\n",
    "\t\ttopbin_check: bool = True\n",
    ") -> Tuple[np.ndarray, int, np.ndarray, float]:\n",
    "\tfft_len = 2 ** _nextpow2(np.ceil(q * fs / fmin))\n",
    "\n",
    "\tif (fmax is not None) and (n_bins is None):\n",
    "\t\tn_bins = np.ceil(bins_per_octave * np.log2(fmax / fmin))\n",
    "\t\tfreqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n",
    "\telif (fmax is None) and (n_bins is not None):\n",
    "\t\tfreqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n",
    "\telse:\n",
    "\t\twarnings.warn(\"If nmax is given, n_bins will be ignored\", SyntaxWarning)\n",
    "\t\tn_bins = np.ceil(bins_per_octave * np.log2(fmax / fmin))\n",
    "\t\tfreqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n",
    "\n",
    "\tif np.max(freqs) > fs / 2 and topbin_check:\n",
    "\t\traise ValueError(f\"The top bin {np.max(freqs)} Hz has exceeded the Nyquist frequency, \\\n",
    "                           please reduce the `n_bins`\")\n",
    "\n",
    "\tkernel = np.zeros((int(n_bins), int(fft_len)), dtype=np.complex64)\n",
    "\n",
    "\tlength = np.ceil(q * fs / freqs)\n",
    "\tfor k in range(0, int(n_bins)):\n",
    "\t\tfreq = freqs[k]\n",
    "\t\tl = np.ceil(q * fs / freq)\n",
    "\n",
    "\t\tif l % 2 == 1:\n",
    "\t\t\tstart = int(np.ceil(fft_len / 2.0 - l / 2.0)) - 1\n",
    "\t\telse:\n",
    "\t\t\tstart = int(np.ceil(fft_len / 2.0 - l / 2.0))\n",
    "\n",
    "\t\tsig = get_window(window, int(l), fftbins=True) * np.exp(\n",
    "\t\t\tnp.r_[-l // 2:l // 2] * 1j * 2 * np.pi * freq / fs) / l\n",
    "\n",
    "\t\tif norm:\n",
    "\t\t\tkernel[k, start:start + int(l)] = sig / np.linalg.norm(sig, norm)\n",
    "\t\telse:\n",
    "\t\t\tkernel[k, start:start + int(l)] = sig\n",
    "\treturn kernel, fft_len, length, freqs\n",
    "\n",
    "\n",
    "def _nextpow2(a: float) -> int:\n",
    "\treturn int(np.ceil(np.log2(a)))\n",
    "\n",
    "\n",
    "# Function to prepare cqt kernel\n",
    "def prepare_cqt_kernel(\n",
    "\t\tsr=22050,\n",
    "\t\thop_length=512,\n",
    "\t\tfmin=32.70,\n",
    "\t\tfmax=None,\n",
    "\t\tn_bins=84,\n",
    "\t\tbins_per_octave=12,\n",
    "\t\tnorm=1,\n",
    "\t\tfilter_scale=1,\n",
    "\t\twindow=\"hann\"\n",
    "):\n",
    "\tq = float(filter_scale) / (2 ** (1 / bins_per_octave) - 1)\n",
    "\tprint(q)\n",
    "\treturn create_cqt_kernels(q, sr, fmin, n_bins, bins_per_octave, norm, window, fmax)\n",
    "\n",
    "\n",
    "# Function to create cqt image\n",
    "from CWT.cwt import ComplexMorletCWT\n",
    "\n",
    "cwt_transform = ComplexMorletCWT(wavelet_width=8, fs=2048, lower_freq=20, upper_freq=1024, n_scales=IMAGE_SIZE[0],\n",
    "                                 stride=int(np.ceil(4096 / IMAGE_SIZE[0])), output='magnitude',\n",
    "                                 data_format='channels_first')\n",
    "\n",
    "\n",
    "def create_cqt_image(wave, hop_length=16):\n",
    "\tCQTs = []\n",
    "\n",
    "\tCQT = cwt_transform(tf.expand_dims(wave, axis=0))\n",
    "\tCQTs.append(CQT)\n",
    "\treturn tf.convert_to_tensor(CQTs)\n",
    "\n",
    "\n",
    "HOP_LENGTH = 6\n",
    "cqt_kernels, KERNEL_WIDTH, lengths, _ = prepare_cqt_kernel(\n",
    "\tsr=2048,\n",
    "\thop_length=HOP_LENGTH,\n",
    "\tfmin=20,\n",
    "\tfmax=1024,\n",
    "\tbins_per_octave=9)\n",
    "LENGTHS = tf.constant(lengths, dtype=tf.float32)\n",
    "CQT_KERNELS_REAL = tf.constant(np.swapaxes(cqt_kernels.real[:, np.newaxis, :], 0, 2))\n",
    "CQT_KERNELS_IMAG = tf.constant(np.swapaxes(cqt_kernels.imag[:, np.newaxis, :], 0, 2))\n",
    "PADDING = tf.constant([[0, 0],\n",
    "                       [KERNEL_WIDTH // 2, KERNEL_WIDTH // 2],\n",
    "                       [0, 0]])\n"
   ],
   "id": "b7dc991a",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12.490672763062207\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: SyntaxWarning: If nmax is given, n_bins will be ignored\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb44cdfa",
    "outputId": "fce41382-14a8-4cbc-8d25-2758bf919fa6"
   },
   "source": [
    "\n",
    "class GeMPoolingLayer(tf.keras.layers.Layer):\n",
    "\tdef __init__(self, p=1., train_p=False):\n",
    "\t\tsuper().__init__()\n",
    "\t\tif train_p:\n",
    "\t\t\tself.p = tf.Variable(p, dtype=tf.float32)\n",
    "\t\telse:\n",
    "\t\t\tself.p = p\n",
    "\t\tself.eps = 1e-6\n",
    "\n",
    "\tdef call(self, inputs: tf.Tensor, **kwargs):\n",
    "\t\tinputs = tf.clip_by_value(inputs, clip_value_min=1e-6, clip_value_max=tf.reduce_max(inputs))\n",
    "\t\tinputs = tf.pow(inputs, self.p)\n",
    "\t\tinputs = tf.reduce_mean(inputs, axis=[1, 2], keepdims=False)\n",
    "\t\tinputs = tf.pow(inputs, 1. / self.p)\n",
    "\t\treturn inputs\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "\trandom.seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\ttf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "# Function to prepare image\n",
    "def prepare_image(wave, dim=512):\n",
    "\t# Decode raw\n",
    "\twave = tf.reshape(tf.io.decode_raw(wave, tf.float64), (3, 4096))\n",
    "\tscaling = tf.constant([1.5e-20, 1.5e-20, 0.5e-20], dtype=tf.float64)\n",
    "\n",
    "\tnormalized_waves = []\n",
    "\t# Normalize\n",
    "\tfor i in range(3):\n",
    "\t\tnormalized_wave = wave[i] / scaling[i]\n",
    "\t\tnormalized_waves.append(normalized_wave)\n",
    "\twave = tf.stack(normalized_waves)\n",
    "\twave = tf.cast(wave, tf.float32)\n",
    "\timage = create_cqt_image(wave, HOP_LENGTH)\n",
    "\timage = tf.transpose(image[0, 0, :, :, :])\n",
    "\timage = tf.image.resize(image, size=(dim, dim))\n",
    "\treturn tf.reshape(image, (dim, dim, 3))\n",
    "\n",
    "\n",
    "# This function parse our images and also get the target variable\n",
    "def read_labeled_tfrecord(example):\n",
    "\tLABELED_TFREC_FORMAT = {\n",
    "\t\t'wave': tf.io.FixedLenFeature([], tf.string),\n",
    "\t\t'wave_id': tf.io.FixedLenFeature([], tf.string),\n",
    "\t\t'target': tf.io.FixedLenFeature([], tf.int64)\n",
    "\t}\n",
    "\texample = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "\timage = prepare_image(example['wave'])\n",
    "\timage_id = example['wave_id']\n",
    "\ttarget = tf.cast(example['target'], tf.float32)\n",
    "\treturn image, image_id, target\n",
    "\n",
    "\n",
    "# This function parse our images and also get the target variable\n",
    "def read_unlabeled_tfrecord(example):\n",
    "\tLABELED_TFREC_FORMAT = {\n",
    "\t\t'wave': tf.io.FixedLenFeature([], tf.string),\n",
    "\t\t'wave_id': tf.io.FixedLenFeature([], tf.string)\n",
    "\t}\n",
    "\texample = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "\timage = prepare_image(example['wave'])\n",
    "\timage_id = example['wave_id']\n",
    "\treturn image, image_id\n",
    "\n",
    "\n",
    "# This function loads TF Records and parse them into tensors\n",
    "def load_dataset(filenames, ordered=False, labeled=True):\n",
    "\tignore_order = tf.data.Options()\n",
    "\tif not ordered:\n",
    "\t\tignore_order.experimental_deterministic = False\n",
    "\n",
    "\tdataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "\tdataset = dataset.with_options(ignore_order)\n",
    "\tdataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "# This function is to get our training dataset\n",
    "def get_training_dataset(filenames, ordered=False, labeled=True):\n",
    "\tdataset = load_dataset(filenames, ordered=ordered, labeled=labeled)\n",
    "\tdataset = dataset.repeat()\n",
    "\tdataset = dataset.shuffle(2048)\n",
    "\tdataset = dataset.batch(BATCH_SIZE)\n",
    "\tdataset = dataset.prefetch(AUTO)\n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "# This function is to get our validation and test dataset\n",
    "def get_val_test_dataset(filenames, ordered=True, labeled=True):\n",
    "\tdataset = load_dataset(filenames, ordered=ordered, labeled=labeled)\n",
    "\tdataset = dataset.batch(BATCH_SIZE)\n",
    "\tdataset = dataset.prefetch(AUTO)\n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "# Function to count how many photos we have in\n",
    "def count_data_items(filenames):\n",
    "\t# The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "\tn = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "\treturn np.sum(n)\n",
    "\n",
    "\n",
    "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
    "NUM_TESTING_IMAGES = count_data_items(TESTING_FILENAMES)\n",
    "print(f'Dataset: {NUM_TRAINING_IMAGES} training images')\n",
    "print(f'Dataset: {NUM_TESTING_IMAGES} testing images')\n"
   ],
   "id": "cb44cdfa",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset: 560000 training images\n",
      "Dataset: 226000 testing images\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0cdb375b",
    "outputId": "f28cac6c-aeeb-410b-c148-01b62a21a28e"
   },
   "source": [
    "\n",
    "\n",
    "class SAM():\n",
    "\tdef __init__(self, base_optimizer, rho=0.05):\n",
    "\t\tassert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "\t\tself.rho = rho\n",
    "\t\tself.base_optimizer = base_optimizer\n",
    "\n",
    "\tdef first_step(self, gradients, trainable_variables):\n",
    "\t\tself.e_ws = []\n",
    "\t\tgrad_norm = tf.linalg.global_norm(gradients)\n",
    "\t\tfor i in range(len(trainable_variables)):\n",
    "\t\t\te_w = gradients[i] * self.rho / (grad_norm + 1e-12)\n",
    "\t\t\ttrainable_variables[i].assign_add(e_w)\n",
    "\t\t\tself.e_ws.append(e_w)\n",
    "\n",
    "\tdef second_step(self, gradients, trainable_variables):\n",
    "\t\tfor i in range(len(trainable_variables)):\n",
    "\t\t\ttrainable_variables[i].assign_add(-self.e_ws[i])\n",
    "\t\t# do the actual \"sharpness-aware\" update\n",
    "\t\tself.base_optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "\n",
    "# if you want to use model.fit(), override the train_step method of a model with this function, example is mnist_example_keras_fit.\n",
    "# for customization see https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit/\n",
    "def sam_train_step(self, data, rho=0.05, eps=1e-12):\n",
    "\t# Unpack the data. Its structure depends on your model and\n",
    "\t# on what you pass to `fit()`.\n",
    "\tif len(data) == 3:\n",
    "\t\tx, y, sample_weight = data\n",
    "\telse:\n",
    "\t\tsample_weight = None\n",
    "\t\tx, y = data\n",
    "\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\ty_pred = self(x, training=True)  # Forward pass\n",
    "\t\t# Compute the loss value\n",
    "\t\t# (the loss function is configured in `compile()`)\n",
    "\t\tloss = self.compiled_loss(y, y_pred, sample_weight=sample_weight, regularization_losses=self.losses)\n",
    "\n",
    "\t# Compute gradients\n",
    "\ttrainable_vars = self.trainable_variables\n",
    "\tgradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "\t# first step\n",
    "\te_ws = []\n",
    "\tgrad_norm = tf.linalg.global_norm(gradients)\n",
    "\tfor i in range(len(trainable_vars)):\n",
    "\t\te_w = gradients[i] * rho / (grad_norm + eps)\n",
    "\t\ttrainable_vars[i].assign_add(e_w)\n",
    "\t\te_ws.append(e_w)\n",
    "\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\ty_pred = self(x, training=True)  # Forward pass\n",
    "\t\t# Compute the loss value\n",
    "\t\t# (the loss function is configured in `compile()`)\n",
    "\t\tloss = self.compiled_loss(y, y_pred, sample_weight=sample_weight, regularization_losses=self.losses)\n",
    "\n",
    "\ttrainable_vars = self.trainable_variables\n",
    "\tgradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "\tfor i in range(len(trainable_vars)):\n",
    "\t\ttrainable_vars[i].assign_sub(e_ws[i])\n",
    "\tself.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "\t# Update the metrics.\n",
    "\t# Metrics are configured in `compile()`.\n",
    "\tself.compiled_metrics.update_state(y, y_pred, sample_weight=sample_weight)\n",
    "\n",
    "\t# Return a dict mapping metric names to current value.\n",
    "\t# Note that it will include the loss (tracked in self.metrics).\n",
    "\treturn {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "# Learning rate callback function\n",
    "def get_lr_callback():\n",
    "\tlr_start = 0.0001\n",
    "\tlr_max = 0.000015 * BATCH_SIZE\n",
    "\tlr_min = 0.0000001\n",
    "\tlr_ramp_ep = 3\n",
    "\tlr_sus_ep = 0\n",
    "\tlr_decay = 0.7\n",
    "\n",
    "\tdef lrfn(epoch):\n",
    "\t\tif epoch < lr_ramp_ep:\n",
    "\t\t\tlr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "\t\telif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "\t\t\tlr = lr_max\n",
    "\t\telse:\n",
    "\t\t\tlr = (lr_max - lr_min) * lr_decay ** (epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "\t\treturn lr\n",
    "\n",
    "\tlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=VERBOSE)\n",
    "\treturn lr_callback\n",
    "\n",
    "\n",
    "import math\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "class CosineAnnealingScheduler(Callback):\n",
    "\t\"\"\"Cosine annealing scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "\tdef __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
    "\t\tsuper(CosineAnnealingScheduler, self).__init__()\n",
    "\t\tself.T_max = T_max\n",
    "\t\tself.eta_max = eta_max\n",
    "\t\tself.eta_min = eta_min\n",
    "\t\tself.verbose = verbose\n",
    "\n",
    "\tdef on_epoch_begin(self, epoch, logs=None):\n",
    "\t\tif not hasattr(self.model.optimizer, 'lr'):\n",
    "\t\t\traise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "\t\tlr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
    "\t\tK.set_value(self.model.optimizer.lr, lr)\n",
    "\t\tif self.verbose > 0:\n",
    "\t\t\tprint('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
    "\t\t\t      'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "\tdef on_epoch_end(self, epoch, logs=None):\n",
    "\t\tlogs = logs or {}\n",
    "\t\tlogs['lr'] = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "\n",
    "decay = CosineAnnealingScheduler(T_max=100, eta_max=1e-3, eta_min=1e-6)\n",
    "\n",
    "\n",
    "# Function to create our EfficientNetB7 model\n",
    "!pip install -U git+https: // github.com / leondgarse / keras_efficientnet_v2\n",
    "import keras_efficientnet_v2\n",
    "\n",
    "\n",
    "def get_model():\n",
    "\twith strategy.scope():\n",
    "\t\tinp = tf.keras.layers.Input(shape=(*IMAGE_SIZE, 3))\n",
    "\t\tx = efn.EfficientNetB7(include_top=False, weights='noisy-student')(inp)\n",
    "\t\tx = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\t\toutput = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\t\tmodel = tf.keras.models.Model(inputs=[inp], outputs=[output])\n",
    "\t\topt = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "\t\topt = tfa.optimizers.SWA(opt)\n",
    "\t\tmodel.compile(\n",
    "\t\t\toptimizer=opt,\n",
    "\t\t\tloss=[tf.keras.losses.BinaryCrossentropy()],\n",
    "\t\t\tmetrics=[tf.keras.metrics.AUC()]\n",
    "\t\t)\n",
    "\t\treturn model\n",
    "\n",
    "\n",
    "options = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "\tfilepath=\"Temp.h5\",\n",
    "\tsave_weights_only=True,\n",
    "\tmonitor='auc',\n",
    "\tmode='max',\n",
    "\tsave_best_only=True, options=options)\n",
    "\n",
    "\n",
    "# Function to train a model with 100% of the data\n",
    "def train_and_evaluate(SEED=42):\n",
    "\tprint('\\n')\n",
    "\tprint('-' * 50)\n",
    "\tprint(f'Training EFFB7 with 100% of the data with seed {SEED} for {EPOCHS} epochs')\n",
    "\tif tpu:\n",
    "\t\ttf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\ttrain_dataset = get_training_dataset(TRAINING_FILENAMES, ordered=False, labeled=True)\n",
    "\ttrain_dataset = train_dataset.map(lambda image, image_id, target: (image, target))\n",
    "\tSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // (BATCH_SIZE * 4)\n",
    "\tK.clear_session()\n",
    "\t# Seed everything\n",
    "\tseed_everything(SEED)\n",
    "\tmodel = get_model()\n",
    "\thistory = model.fit(train_dataset,\n",
    "\t                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "\t                    epochs=EPOCHS,\n",
    "\t                    callbacks=[get_lr_callback(), model_checkpoint_callback],\n",
    "\t                    verbose=1)\n",
    "\n",
    "\tprint('\\n')\n",
    "\tprint('-' * 50)\n",
    "\tprint('Test inference...')\n",
    "\tmodel = model.load_weights(\"content/Temp\", options=options)\n",
    "\t# Predict the test set\n",
    "\tdataset = get_val_test_dataset(TESTING_FILENAMES, ordered=True, labeled=False)\n",
    "\timage = dataset.map(lambda image, image_id: image)\n",
    "\ttest_predictions = model.predict(image).astype(np.float32).reshape(-1)\n",
    "\t# Get the test set image_id\n",
    "\timage_id = dataset.map(lambda image, image_id: image_id).unbatch()\n",
    "\timage_id = next(iter(image_id.batch(NUM_TESTING_IMAGES))).numpy().astype('U')\n",
    "\t# Create dataframe output\n",
    "\ttest_df = pd.DataFrame({'id': image_id, 'target': test_predictions})\n",
    "\t# Save test dataframe to disk\n",
    "\ttest_df.to_csv(f'TEST_xEfficientNetB7_{IMAGE_SIZE[0]}_{SEED}.csv', index=False)\n",
    "\n",
    "\n",
    "train_and_evaluate(SEED=2020)\n",
    "g\n"
   ],
   "id": "0cdb375b",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting git+https://github.com/leondgarse/keras_efficientnet_v2\n",
      "  Cloning https://github.com/leondgarse/keras_efficientnet_v2 to /tmp/pip-req-build-luntt1yz\n",
      "  Running command git clone -q https://github.com/leondgarse/keras_efficientnet_v2 /tmp/pip-req-build-luntt1yz\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-efficientnet-v2==1.1.4) (2.6.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (3.7.4.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (3.17.3)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (1.40.0)\n",
      "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (5.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (1.1.2)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (0.37.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (3.1.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (0.12.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (3.3.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (2.6.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (1.6.3)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (2.6.0)\n",
      "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (2.6.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-efficientnet-v2==1.1.4) (1.12)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow->keras-efficientnet-v2==1.1.4) (1.5.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (57.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (2.23.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (4.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow->keras-efficientnet-v2==1.1.4) (3.5.0)\n",
      "Building wheels for collected packages: keras-efficientnet-v2\n",
      "  Building wheel for keras-efficientnet-v2 (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for keras-efficientnet-v2: filename=keras_efficientnet_v2-1.1.4-py3-none-any.whl size=30996 sha256=954292f9b0cd3a900023ed439d547af7ff816cc844981eee16006456493e8be5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-y1fh7sg2/wheels/2b/03/e6/76f640aa248f325b4dfacff31f33fd84d12806b10cabd9e5e9\n",
      "Successfully built keras-efficientnet-v2\n",
      "Installing collected packages: keras-efficientnet-v2\n",
      "Successfully installed keras-efficientnet-v2-1.1.4\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-a950bd59393b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 153\u001B[0;31m \u001B[0moptions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCheckpointOptions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexperimental_io_device\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"/job:localhost\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    154\u001B[0m model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n\u001B[1;32m    155\u001B[0m         \u001B[0mfilepath\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"Temp.h5\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tf' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Chm0dAstZWRF"
   },
   "source": [
    "import os\n",
    "from tensorflow.python.profiler import profiler_client\n",
    "\n",
    "tpu_profile_service_address = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466')\n",
    "print(profiler_client.monitor(tpu_profile_service_address, 100, 2))"
   ],
   "id": "Chm0dAstZWRF",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m6efo5dSdwHY"
   },
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ],
   "id": "m6efo5dSdwHY",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vW9o1ME7d7Rk"
   },
   "source": [
    "!wget \"https://storage.googleapis.com/kaggle-data-sets/1533418/2530570/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210918%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210918T071602Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=4ad78c354ca1f96f9b33dfa09f907498da54e4345797b258d8737515a8c1d9d86101bd64d8fcaf431450d292e3da876f456c18679ecfc8f6333ca3afa0bd4404efdf2de4dfeab2334e2ec9286a81dd72dbfb44128f8fc7ec188efb0e6791d153845edccde1e984771634de8577294a78dd0266568ecf8e233f66cf8b644aa0efa484634eb81b9cf9555f088660825931b920c0744555195b9bd0dc12e5be8fefa430d31daa71d4132a8c345e07cb7ac3aaec84ee9a0a266bc4067ac52079799c1f78db7fdab55ffbd417d571f2c69f96d3c31eb341b849bb62d4dd39d16f68e1b5d7268bc7a0558d95651834f43cb2f1d1b759d625fa1130761b3fdffb2294f2\""
   ],
   "id": "vW9o1ME7d7Rk",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3pqfD7Lww9ah"
   },
   "source": [
    "!unzip \"/content/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com@kaggle-161607.iam.gserviceaccount.com%2F20210918%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210918T071602Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=hos\""
   ],
   "id": "3pqfD7Lww9ah",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ybq-tY3Uainn"
   },
   "source": [
    "model = get_model()\n",
    "model.load_weights(\"/content/Temp.h5\", options=options)\n",
    "model.summary()\n"
   ],
   "id": "ybq-tY3Uainn",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kDM3mcYHsU4w"
   },
   "source": [
    "fold0 = []\n",
    "fold1 = []\n",
    "fold2 = []\n",
    "fold3 = []\n",
    "dataset = get_val_test_dataset(TESTING_FILENAMES, ordered=True, labeled=False)\n",
    "image = dataset.map(lambda image, image_id: image)\n",
    "for i in range(4):\n",
    "\tmodel.load_weights(f\"/content/drive/MyDrive/models/CQTwithoutbandpass/fold{i}.h5\")\n",
    "\ttest_predictions = model.predict(image, verbose=1).astype(np.float32).reshape(-1)\n",
    "\tif i == 0:\n",
    "\t\tfold0.append(test_predictions)\n",
    "\tif i == 1:\n",
    "\t\tfold1.append(test_predictions)\n",
    "\tif i == 2:\n",
    "\t\tfold2.append(test_predictions)\n",
    "\tif i == 3:\n",
    "\t\tfold3.append(test_predictions)\n",
    "# Get the test set image_id\n",
    "image_id = dataset.map(lambda image, image_id: image_id).unbatch()\n",
    "image_id = next(iter(image_id.batch(NUM_TESTING_IMAGES))).numpy().astype('U')\n",
    "# Create dataframe output\n",
    "test_df = pd.DataFrame({'id': image_id, 'target': (fold0 + fold1 + fold2 + fold3) / 4})\n",
    "# Save test dataframe to disk\n",
    "test_df.to_csv(f'TEST_EfficientNetB7_{IMAGE_SIZE[0]}_{SEED}.csv', index=False)\n"
   ],
   "id": "kDM3mcYHsU4w",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ce8p7J24oKZq"
   },
   "source": [
    "!gsutil cp -r"
   ],
   "id": "Ce8p7J24oKZq",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lqPm1ZyJlKPZ"
   },
   "source": [
    "print(f'TEST_EfficientNetB7_{IMAGE_SIZE[0]}_{SEED}.csv')"
   ],
   "id": "lqPm1ZyJlKPZ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t7xxwwwkoivi"
   },
   "source": [
    "!cp -r \"TEST_EfficientNetB7_512_1991.csv\" \"/content/drive/MyDrive/models\""
   ],
   "id": "t7xxwwwkoivi",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6KZOGP-Mnw1Y"
   },
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ],
   "id": "6KZOGP-Mnw1Y",
   "execution_count": null,
   "outputs": []
  }
 ]
}