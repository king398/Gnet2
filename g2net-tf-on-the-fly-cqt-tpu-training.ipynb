{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## About this notebook\n\nThis notebook is based on [CQT G2Net EfficientNetB1[TPU Training]](https://www.kaggle.com/miklgr500/cqt-g2net-efficientnetb7-tpu-training-w-b) by [Welf Crozzo](https://www.kaggle.com/miklgr500) and [nnAudio Constant Q-transform Demonstration](https://www.kaggle.com/atamazian/nnaudio-constant-q-transform-demonstration) by [Araik Tamazian](https://www.kaggle.com/atamazian).\n\nThis notebook use Constant Q-Transform for feature extraction and EfficientNetB0 for classification. The whole pipeline is implemented with Tensorflow, and the training process runs on TPU.\n\nThe main difference between this notebook and Welf's notebook is the use of on-the-fly CQT computation implemented with Tensorflow, which is similar to the idea of [nnAudio](https://github.com/KinWaiCheuk/nnAudio)'s [CQT1992v2](https://kinwaicheuk.github.io/nnAudio/_autosummary/nnAudio.Spectrogram.CQT1992v2.html?highlight=cqt1992v2#nnAudio.Spectrogram.CQT1992v2) layer.\n\n* [Inference Notebook](https://www.kaggle.com/hidehisaarai1213/g2net-tf-on-the-fly-cqt-tpu-inference)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Install Dependencies",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\n!pip install efficientnet tensorflow_addons > /dev/null\n!pip install -q git+https://github.com//Kevin-McIsaac/cmorlet-tensorflow@Performance --no-deps\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:37:47.433223Z",
     "iopub.execute_input": "2021-09-08T16:37:47.433687Z",
     "iopub.status.idle": "2021-09-08T16:38:01.553047Z",
     "shell.execute_reply.started": "2021-09-08T16:37:47.433581Z",
     "shell.execute_reply": "2021-09-08T16:38:01.551776Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import os\nimport math\nimport random\nimport re\nimport warnings\nfrom pathlib import Path\nfrom typing import Optional, Tuple\n\nimport efficientnet.tfkeras as efn\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom kaggle_datasets import KaggleDatasets\nfrom scipy.signal import get_window\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:01.55489Z",
     "iopub.execute_input": "2021-09-08T16:38:01.555249Z",
     "iopub.status.idle": "2021-09-08T16:38:08.986134Z",
     "shell.execute_reply.started": "2021-09-08T16:38:01.55521Z",
     "shell.execute_reply": "2021-09-08T16:38:08.984922Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "tf.__version__",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:08.997528Z",
     "iopub.execute_input": "2021-09-08T16:38:08.997897Z",
     "iopub.status.idle": "2021-09-08T16:38:09.006933Z",
     "shell.execute_reply.started": "2021-09-08T16:38:08.997869Z",
     "shell.execute_reply": "2021-09-08T16:38:09.005909Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Config",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "NUM_FOLDS = 4\nIMAGE_SIZE = 256\nBATCH_SIZE = 32\nEFFICIENTNET_SIZE = 7\nWEIGHTS = \"imagenet\"\n\nMIXUP_PROB = 0.0\nEPOCHS = 20\nR_ANGLE = 0 / 180 * np.pi\nS_SHIFT = 0.0\nT_SHIFT = 0.0\nLABEL_POSITIVE_SHIFT = 0.99",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:09.008308Z",
     "iopub.execute_input": "2021-09-08T16:38:09.008644Z",
     "iopub.status.idle": "2021-09-08T16:38:09.020058Z",
     "shell.execute_reply.started": "2021-09-08T16:38:09.008591Z",
     "shell.execute_reply": "2021-09-08T16:38:09.018864Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "SAVEDIR = Path(\"models\")\nSAVEDIR.mkdir(exist_ok=True)\n\nOOFDIR = Path(\"oof\")\nOOFDIR.mkdir(exist_ok=True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:09.022815Z",
     "iopub.execute_input": "2021-09-08T16:38:09.023126Z",
     "iopub.status.idle": "2021-09-08T16:38:09.032225Z",
     "shell.execute_reply.started": "2021-09-08T16:38:09.023097Z",
     "shell.execute_reply": "2021-09-08T16:38:09.03099Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Utilities",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def set_seed(seed=42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n\nset_seed(21)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:09.034408Z",
     "iopub.execute_input": "2021-09-08T16:38:09.034765Z",
     "iopub.status.idle": "2021-09-08T16:38:09.045863Z",
     "shell.execute_reply.started": "2021-09-08T16:38:09.034733Z",
     "shell.execute_reply": "2021-09-08T16:38:09.044923Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def auto_select_accelerator():\n    TPU_DETECTED = False\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n        TPU_DETECTED = True\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n\n    return strategy, TPU_DETECTED",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:09.047273Z",
     "iopub.execute_input": "2021-09-08T16:38:09.047923Z",
     "iopub.status.idle": "2021-09-08T16:38:09.059952Z",
     "shell.execute_reply.started": "2021-09-08T16:38:09.047878Z",
     "shell.execute_reply": "2021-09-08T16:38:09.058959Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "strategy, tpu_detected = auto_select_accelerator()\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:09.061276Z",
     "iopub.execute_input": "2021-09-08T16:38:09.061889Z",
     "iopub.status.idle": "2021-09-08T16:38:14.726596Z",
     "shell.execute_reply.started": "2021-09-08T16:38:09.061851Z",
     "shell.execute_reply": "2021-09-08T16:38:14.725484Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Loading",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "gcs_paths = []\nfor i, j in [(0, 4), (5, 9), (10, 14), (15, 19)]:\n    GCS_path = KaggleDatasets().get_gcs_path(f\"g2net-waveform-tfrecords-train-{i}-{j}\")\n    gcs_paths.append(GCS_path)\n    print(GCS_path)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:14.72797Z",
     "iopub.execute_input": "2021-09-08T16:38:14.728289Z",
     "iopub.status.idle": "2021-09-08T16:38:16.167439Z",
     "shell.execute_reply.started": "2021-09-08T16:38:14.728258Z",
     "shell.execute_reply": "2021-09-08T16:38:16.166368Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "all_files = []\nfor path in gcs_paths:\n    all_files.extend(np.sort(np.array(tf.io.gfile.glob(path + \"/train*.tfrecords\"))))\n\nprint(\"train_files: \", len(all_files))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:16.168826Z",
     "iopub.execute_input": "2021-09-08T16:38:16.169124Z",
     "iopub.status.idle": "2021-09-08T16:38:16.49346Z",
     "shell.execute_reply.started": "2021-09-08T16:38:16.169095Z",
     "shell.execute_reply": "2021-09-08T16:38:16.492422Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(gcs_paths)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:16.494886Z",
     "iopub.execute_input": "2021-09-08T16:38:16.495194Z",
     "iopub.status.idle": "2021-09-08T16:38:16.500632Z",
     "shell.execute_reply.started": "2021-09-08T16:38:16.495163Z",
     "shell.execute_reply": "2021-09-08T16:38:16.499796Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Dataset Preparation\n\nHere's the main contribution of this notebook - Tensorflow version of on-the-fly CQT computation. Note that some of the operations used in CQT computation are not supported by TPU, therefore the implementation is not a TF layer but a function that runs on CPU.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def create_cqt_kernels(\n    q: float,\n    fs: float,\n    fmin: float,\n    n_bins: int = 84,\n    bins_per_octave: int = 12,\n    norm: float = 1,\n    window: str = \"hann\",\n    fmax: Optional[float] = None,\n    topbin_check: bool = True\n) -> Tuple[np.ndarray, int, np.ndarray, float]:\n    fft_len = 2 ** _nextpow2(np.ceil(q * fs / fmin))\n    \n    if (fmax is not None) and (n_bins is None):\n        n_bins = np.ceil(bins_per_octave * np.log2(fmax / fmin))\n        freqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n    elif (fmax is None) and (n_bins is not None):\n        freqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n    else:\n        warnings.warn(\"If nmax is given, n_bins will be ignored\", SyntaxWarning)\n        n_bins = np.ceil(bins_per_octave * np.log2(fmax / fmin))\n        freqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n        \n    if np.max(freqs) > fs / 2 and topbin_check:\n        raise ValueError(f\"The top bin {np.max(freqs)} Hz has exceeded the Nyquist frequency, \\\n                           please reduce the `n_bins`\")\n    \n    kernel = np.zeros((int(n_bins), int(fft_len)), dtype=np.complex64)\n    \n    length = np.ceil(q * fs / freqs)\n    for k in range(0, int(n_bins)):\n        freq = freqs[k]\n        l = np.ceil(q * fs / freq)\n        \n        if l % 2 == 1:\n            start = int(np.ceil(fft_len / 2.0 - l / 2.0)) - 1\n        else:\n            start = int(np.ceil(fft_len / 2.0 - l / 2.0))\n\n        sig = get_window(window, int(l), fftbins=True) * np.exp(\n            np.r_[-l // 2:l // 2] * 1j * 2 * np.pi * freq / fs) / l\n        \n        if norm:\n            kernel[k, start:start + int(l)] = sig / np.linalg.norm(sig, norm)\n        else:\n            kernel[k, start:start + int(l)] = sig\n    return kernel, fft_len, length, freqs\n\n\ndef _nextpow2(a: float) -> int:\n    return int(np.ceil(np.log2(a)))\n\n\ndef prepare_cqt_kernel(\n    sr=22050,\n    hop_length=512,\n    fmin=32.70,\n    fmax=None,\n    n_bins=84,\n    bins_per_octave=12,\n    norm=1,\n    filter_scale=1,\n    window=\"hann\"\n):\n    q = float(filter_scale) / (2 ** (1 / bins_per_octave) - 1)\n    print(q)\n    return create_cqt_kernels(q, sr, fmin, n_bins, bins_per_octave, norm, window, fmax)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:16.501737Z",
     "iopub.execute_input": "2021-09-08T16:38:16.502031Z",
     "iopub.status.idle": "2021-09-08T16:38:16.522659Z",
     "shell.execute_reply.started": "2021-09-08T16:38:16.502003Z",
     "shell.execute_reply": "2021-09-08T16:38:16.52155Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "HOP_LENGTH = 4\ncqt_kernels, KERNEL_WIDTH, lengths, _ = prepare_cqt_kernel(\n    sr=2048,\n    hop_length=HOP_LENGTH,\n    fmin=20,\n    fmax=500,\n    bins_per_octave=24)\nLENGTHS = tf.constant(lengths, dtype=tf.float32)\nCQT_KERNELS_REAL = tf.constant(np.swapaxes(cqt_kernels.real[:, np.newaxis, :], 0, 2))\nCQT_KERNELS_IMAG = tf.constant(np.swapaxes(cqt_kernels.imag[:, np.newaxis, :], 0, 2))\nPADDING = tf.constant([[0, 0],\n                        [KERNEL_WIDTH // 2, KERNEL_WIDTH // 2],\n                        [0, 0]])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:16.524068Z",
     "iopub.execute_input": "2021-09-08T16:38:16.524403Z",
     "iopub.status.idle": "2021-09-08T16:38:16.588422Z",
     "shell.execute_reply.started": "2021-09-08T16:38:16.524371Z",
     "shell.execute_reply": "2021-09-08T16:38:16.586981Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from CWT.cwt import ComplexMorletCWT\n\ncwt_transform = ComplexMorletCWT(wavelet_width=8, fs=2048, lower_freq=20, upper_freq=500, n_scales=IMAGE_SIZE,\n                                 stride=int(np.ceil(4096 / IMAGE_SIZE)), output='magnitude',\n                                 data_format='channels_first')\n\n\ndef create_cqt_image(wave, hop_length=16):\n\tCQTs = []\n\n\n\n\tCQT = cwt_transform(tf.expand_dims(wave,axis=0))\n\tCQTs.append(CQT)\n\treturn tf.convert_to_tensor(CQTs)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:16.590205Z",
     "iopub.execute_input": "2021-09-08T16:38:16.590655Z",
     "iopub.status.idle": "2021-09-08T16:38:17.210867Z",
     "shell.execute_reply.started": "2021-09-08T16:38:16.590585Z",
     "shell.execute_reply": "2021-09-08T16:38:17.209757Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def read_labeled_tfrecord(example):\n    tfrec_format = {\n        \"wave\": tf.io.FixedLenFeature([], tf.string),\n        \"wave_id\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    }\n    example = tf.io.parse_single_example(example, tfrec_format)\n    return prepare_image(example[\"wave\"], IMAGE_SIZE), tf.reshape(tf.cast(example[\"target\"], tf.float32), [1])\n\n\ndef read_unlabeled_tfrecord(example, return_image_id):\n    tfrec_format = {\n        \"wave\": tf.io.FixedLenFeature([], tf.string),\n        \"wave_id\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrec_format)\n    return prepare_image(example[\"wave\"], IMAGE_SIZE), example[\"wave_id\"] if return_image_id else 0\n\n\ndef count_data_items(fileids):\n    return len(fileids) * 28000\n\n\ndef count_data_items_test(fileids):\n    return len(fileids) * 22600\n\n\ndef mixup(image, label, probability=0.5, aug_batch=64 * 8):\n    imgs = []\n    labs = []\n    for j in range(aug_batch):\n        p = tf.cast(tf.random.uniform([], 0, 1) <= probability, tf.float32)\n        k = tf.cast(tf.random.uniform([], 0, aug_batch), tf.int32)\n        a = tf.random.uniform([], 0, 1) * p\n\n        img1 = image[j]\n        img2 = image[k]\n        imgs.append((1 - a) * img1 + a * img2)\n        lab1 = label[j]\n        lab2 = label[k]\n        labs.append((1 - a) * lab1 + a * lab2)\n    image2 = tf.reshape(tf.stack(imgs), (aug_batch, IMAGE_SIZE, IMAGE_SIZE, 3))\n    label2 = tf.reshape(tf.stack(labs), (aug_batch,))\n    return image2, label2\n\n\ndef time_shift(img, shift=T_SHIFT):\n    if shift > 0:\n        T = IMAGE_SIZE\n        P = tf.random.uniform([],0,1)\n        SHIFT = tf.cast(T * P, tf.int32)\n        return tf.concat([img[-SHIFT:], img[:-SHIFT]], axis=0)\n    return img\n\n\ndef rotate(img, angle=R_ANGLE):\n    if angle > 0:\n        P = tf.random.uniform([],0,1)\n        A = tf.cast(angle * P, tf.float32)\n        return tfa.image.rotate(img, A)\n    return img\n\n\ndef spector_shift(img, shift=S_SHIFT):\n    if shift > 0:\n        T = IMAGE_SIZE\n        P = tf.random.uniform([],0,1)\n        SHIFT = tf.cast(T * P, tf.int32)\n        return tf.concat([img[:, -SHIFT:], img[:, :-SHIFT]], axis=1)\n    return img\n\ndef img_aug_f(img):\n    img = time_shift(img)\n    img = spector_shift(img)\n    # img = rotate(img)\n    return img\n\n\ndef imgs_aug_f(imgs, batch_size):\n    _imgs = []\n    DIM = IMAGE_SIZE\n    for j in range(batch_size):\n        _imgs.append(img_aug_f(imgs[j]))\n    return tf.reshape(tf.stack(_imgs),(batch_size,DIM,DIM,3))\n\n\ndef label_positive_shift(labels):\n    return labels * LABEL_POSITIVE_SHIFT\n\n\ndef aug_f(imgs, labels, batch_size):\n    imgs, label = mixup(imgs, labels, MIXUP_PROB, batch_size)\n    imgs = imgs_aug_f(imgs, batch_size)\n    return imgs, label_positive_shift(label)\n\n\ndef prepare_image(wave, dim=256):\n    wave = tf.reshape(tf.io.decode_raw(wave, tf.float64), (3, 4096))\n    normalized_waves = []\n    for i in range(3):\n        normalized_wave = wave[i] * 1.3e+22\n        normalized_waves.append(normalized_wave)\n    wave = tf.stack(normalized_waves)\n    wave = tf.cast(wave, tf.float32)\n    image = create_cqt_image(wave, HOP_LENGTH)\n    image = tf.transpose(image[0,0,:,:,:])\n    image = tf.image.resize(image, size=(dim, dim))\n    return tf.reshape(image, (dim, dim, 3))\n\n\ndef get_dataset(files, batch_size=16, repeat=False, shuffle=False, aug=True, labeled=True, return_image_ids=True):\n    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO, compression_type=\"GZIP\")\n    ds = ds.cache()\n\n    if repeat:\n        ds = ds.repeat()\n\n    if shuffle:\n        ds = ds.shuffle(1024 * 2)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n\n    if labeled:\n        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n    else:\n        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_ids), num_parallel_calls=AUTO)\n\n    ds = ds.batch(batch_size * REPLICAS)\n    if aug:\n        ds = ds.map(lambda x, y: aug_f(x, y, batch_size * REPLICAS), num_parallel_calls=AUTO)\n    ds = ds.prefetch(AUTO)\n    return ds",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:17.21351Z",
     "iopub.execute_input": "2021-09-08T16:38:17.213982Z",
     "iopub.status.idle": "2021-09-08T16:38:17.248785Z",
     "shell.execute_reply.started": "2021-09-08T16:38:17.213933Z",
     "shell.execute_reply": "2021-09-08T16:38:17.247684Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def build_model(size=256, efficientnet_size=0, weights=\"imagenet\", count=0):\n    inputs = tf.keras.layers.Input(shape=( 256, 256, 3))\n    \n    efn_string= f\"EfficientNetB{efficientnet_size}\"\n    efn_layer = getattr(efn, efn_string)(input_shape=( 256, 256, 3), weights=weights, include_top=False)\n\n    x = efn_layer(inputs)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n    model = tf.keras.Model(inputs=inputs, outputs=x)\n\n    lr_decayed_fn = tf.keras.experimental.CosineDecay(1e-3, count)\n    opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate=1e-4)\n    loss = tf.keras.losses.BinaryCrossentropy()\n    model.compile(optimizer=opt, loss=loss, metrics=[\"AUC\"])\n    return model",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:17.250294Z",
     "iopub.execute_input": "2021-09-08T16:38:17.250604Z",
     "iopub.status.idle": "2021-09-08T16:38:17.26717Z",
     "shell.execute_reply.started": "2021-09-08T16:38:17.250575Z",
     "shell.execute_reply": "2021-09-08T16:38:17.266024Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def get_lr_callback(batch_size=8, replicas=8):\n    lr_start   = 1e-4\n    lr_max     = 0.000015 * replicas * batch_size\n    lr_min     = 1e-7\n    lr_ramp_ep = 3\n    lr_sus_ep  = 0\n    lr_decay   = 0.7\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n    return lr_callback",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:17.269867Z",
     "iopub.execute_input": "2021-09-08T16:38:17.270673Z",
     "iopub.status.idle": "2021-09-08T16:38:17.284108Z",
     "shell.execute_reply.started": "2021-09-08T16:38:17.270597Z",
     "shell.execute_reply": "2021-09-08T16:38:17.282995Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Training",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=1213)\noof_pred = []\noof_target = []\n\nfiles_train_all = np.array(all_files)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-08T16:38:17.285785Z",
     "iopub.execute_input": "2021-09-08T16:38:17.28629Z",
     "iopub.status.idle": "2021-09-08T16:38:17.29851Z",
     "shell.execute_reply.started": "2021-09-08T16:38:17.286257Z",
     "shell.execute_reply": "2021-09-08T16:38:17.297211Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "for fold, (trn_idx, val_idx) in enumerate(kf.split(files_train_all)):\n\tif fold == 0:\n\t\tfiles_train = files_train_all[trn_idx]\n\t\tfiles_valid = files_train_all[val_idx]\n\n\t\tprint(\"=\" * 120)\n\t\tprint(f\"Fold {fold}\")\n\t\tprint(\"=\" * 120)\n\n\t\ttrain_image_count = count_data_items(files_train)\n\t\tvalid_image_count = count_data_items(files_valid)\n\n\t\ttf.keras.backend.clear_session()\n\t\tstrategy, tpu_detected = auto_select_accelerator()\n\t\twith strategy.scope():\n\t\t\tmodel = build_model(\n\t\t\t\tsize=IMAGE_SIZE,\n\t\t\t\tefficientnet_size=EFFICIENTNET_SIZE,\n\t\t\t\tweights=WEIGHTS,\n\t\t\t\tcount=train_image_count // BATCH_SIZE // REPLICAS // 1)\n\n\t\tmodel_ckpt = tf.keras.callbacks.ModelCheckpoint(\n\t\t\tstr(SAVEDIR / f\"fold{fold}.h5\"), monitor=\"val_auc\", verbose=1, save_best_only=True,\n\t\t\tsave_weights_only=True, mode=\"max\", save_freq=\"epoch\", \n\t\t)\n\n\t\thistory = model.fit(\n\t\t\tget_dataset(files_train, batch_size=BATCH_SIZE, shuffle=True, repeat=True, aug=True),\n\t\t\tepochs=EPOCHS,\n\t\t\tcallbacks=[model_ckpt, get_lr_callback(BATCH_SIZE, REPLICAS)],\n\t\t\tsteps_per_epoch=train_image_count // BATCH_SIZE // REPLICAS // 1,\n\t\t\tvalidation_data=get_dataset(files_valid, batch_size=BATCH_SIZE * 4, repeat=False, shuffle=False, aug=False),\n\t\t\tverbose=1,\n\t\t)\n\n\t\tprint(\"Loading best model...\")\n\t\tmodel.load_weights(str(SAVEDIR / f\"fold{fold}.h5\"), options=options)\n\n\t\tds_valid = get_dataset(files_valid, labeled=False, return_image_ids=False, repeat=True, shuffle=False,\n\t\t                       batch_size=BATCH_SIZE * 2, aug=False)\n\t\tSTEPS = valid_image_count / BATCH_SIZE / 2 / REPLICAS\n\t\tpred = model.predict(ds_valid, steps=STEPS, verbose=1)[:valid_image_count]\n\t\toof_pred.append(np.mean(pred.reshape((valid_image_count, 1), order=\"F\"), axis=1))\n\n\t\tds_valid = get_dataset(files_valid, repeat=False, labeled=True, return_image_ids=True, aug=False)\n\t\toof_target.append(np.array([target.numpy() for img, target in iter(ds_valid.unbatch())]))\n\n\t\tplt.figure(figsize=(8, 6))\n\t\tsns.distplot(oof_pred[-1])\n\t\tplt.show()\n\n\t\tplt.figure(figsize=(15, 5))\n\t\tplt.plot(\n\t\t\tnp.arange(len(history.history[\"auc\"])),\n\t\t\thistory.history[\"auc\"],\n\t\t\t\"-o\",\n\t\t\tlabel=\"Train auc\",\n\t\t\tcolor=\"#ff7f0e\")\n\t\tplt.plot(\n\t\t\tnp.arange(len(history.history[\"auc\"])),\n\t\t\thistory.history[\"val_auc\"],\n\t\t\t\"-o\",\n\t\t\tlabel=\"Val auc\",\n\t\t\tcolor=\"#1f77b4\")\n\n\t\tx = np.argmax(history.history[\"val_auc\"])\n\t\ty = np.max(history.history[\"val_auc\"])\n\n\t\txdist = plt.xlim()[1] - plt.xlim()[0]\n\t\tydist = plt.ylim()[1] - plt.ylim()[0]\n\n\t\tplt.scatter(x, y, s=200, color=\"#1f77b4\")\n\t\tplt.text(x - 0.03 * xdist, y - 0.13 * ydist, f\"max auc\\n{y}\", size=14)\n\n\t\tplt.ylabel(\"auc\", size=14)\n\t\tplt.xlabel(\"Epoch\", size=14)\n\t\tplt.legend(loc=2)\n\n\t\tplt2 = plt.gca().twinx()\n\t\tplt2.plot(\n\t\t\tnp.arange(len(history.history[\"auc\"])),\n\t\t\thistory.history[\"loss\"],\n\t\t\t\"-o\",\n\t\t\tlabel=\"Train Loss\",\n\t\t\tcolor=\"#2ca02c\")\n\t\tplt2.plot(\n\t\t\tnp.arange(len(history.history[\"auc\"])),\n\t\t\thistory.history[\"val_loss\"],\n\t\t\t\"-o\",\n\t\t\tlabel=\"Val Loss\",\n\t\t\tcolor=\"#d62728\")\n\n\t\tx = np.argmin(history.history[\"val_loss\"])\n\t\ty = np.min(history.history[\"val_loss\"])\n\n\t\tydist = plt.ylim()[1] - plt.ylim()[0]\n\n\t\tplt.scatter(x, y, s=200, color=\"#d62728\")\n\t\tplt.text(x - 0.03 * xdist, y + 0.05 * ydist, \"min loss\", size=14)\n\n\t\tplt.ylabel(\"Loss\", size=14)\n\t\tplt.title(f\"Fold {fold + 1} - Image Size {IMAGE_SIZE}, EfficientNetB{EFFICIENTNET_SIZE}\", size=18)\n\n\t\tplt.legend(loc=3)\n\t\tplt.savefig(OOFDIR / f\"fig{fold}.png\")\n\t\tplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-09T03:06:34.135208Z",
     "iopub.execute_input": "2021-09-09T03:06:34.135786Z",
     "iopub.status.idle": "2021-09-09T03:06:34.349247Z",
     "shell.execute_reply.started": "2021-09-09T03:06:34.135696Z",
     "shell.execute_reply": "2021-09-09T03:06:34.346458Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-7fd166d45bb7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfor\u001B[0m \u001B[0mfold\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mtrn_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_idx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiles_train_all\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mfold\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m                 \u001B[0mfiles_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfiles_train_all\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtrn_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m                 \u001B[0mfiles_valid\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfiles_train_all\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mval_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'kf' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'kf' is not defined",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## OOF",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def prepare_image(wave, dim=256):\n    wave = tf.reshape(tf.io.decode_raw(wave, tf.float64), (3, 4096))\n    normalized_waves = []\n    for i in range(3):\n        normalized_wave = wave[i] * 1.3e+22\n        normalized_waves.append(normalized_wave)\n    wave = tf.stack(normalized_waves)\n    wave = tf.cast(wave, tf.float32)\n    image = create_cqt_image(wave, HOP_LENGTH)\n    image = tf.transpose(image[0,0,:,:,:])\n    image = tf.image.resize(image, size=(dim, dim))\n    return tf.reshape(image, (dim, dim, 3))",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# # # ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "oof = np.concatenate(oof_pred)\ntrue = np.concatenate(oof_target)\nauc = roc_auc_score(y_true=true, y_score=oof)\nprint(f\"AUC: {auc:.5f}\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df = pd.DataFrame({\n    \"y_true\": true.reshape(-1),\n    \"y_pred\": oof\n})\ndf.head()",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df.to_csv(OOFDIR / f\"oof.csv\", index=False)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}